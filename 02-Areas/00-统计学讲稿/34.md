大家好，今天呢，我们就进入第11章，这是最后四章当中最重要，也是最后四章当中最难的一章。我们在第九章和第十章分别介绍了分类变量之间关系的测量。分类自变量与数值型因变量之间关系的测量。那这一章呢？我们就介绍数值型变量之间相关关系的测量，那两个数值型变量之间的关系呢？我们分为两种，一种是确定性的关系，又称为函数关系。比如y=fx，那么这个f就是对x的某种处理。我们丢一个特进特定的x进去，
那么就会出来一个对应的y，比如呃，这是圆的。面积公式那圆的面积呢？取决于半径半径确定面积就确定了，确定一个r就会有一个对应的。面积因为这个半径是影响圆的面积的唯一的因素，那么另外一种关系呢？是不确定性关系，比如身高和体重，同样的身高，体重不一定相同。也就是说。你给我一个身高，具体的值我是不能确定体重的，
具体的值的。但是呢，如果身高变高，平均来说啊，体重是上升的，或者说大体上来说啊，如果身高变高，体重是上升的。那也就是说这个x和这个y之间，它是有关系的，但是这个关系并不是确定性的关系。一个特定的x并没有一个特定的y与之对应啊，因为x并不是y的。唯一的影响因素，那么影响体重的因素，
除了身高以外，还有很多啊，后面的这两对xy也是一样的。存在不确定的关系，这是我们关注的关系的类型，不确定性的关系啊，又称为相关关系。但是呢，相关关系呢，又有不同的分类，我们也不是全部都关注，我们通过散点图来看一下。散点图呢，我们在第三章讲过啊，是描述两个数值型变量之间关系的，
通过散点图，我们可以大体观察到变量之间的关系形态，还有关系的强弱。那从形态上来看，第一章的。第一列，第一列的这个两张图。x和y的关系近似的表现为一条直线，那这种关系呢？我们称为线性相关。第二列的两张图。各个观测点呢，都落在了同一条直线上。这种关系呢，我们称为完全相关。
完全相关其实就是确定性关系了啊，就是函数关系了。那么这张图啊，两个变量的关系呢，近似的表现为一条曲线啊，我们就称为非线性相关。呃，或者曲线相关。那么最后一张图，那这个观测点是分散的，没有规律的表示，变量间没有相关关系。不相关。在线性，当线性相关当中啊，
也就是第一列的两张图当中，第一张x和y的变化方向是相同的。使大体上呢，呈现出同增同减。这样一个关系，我们称这种线性相关为。正相关那么第二张图当中呢？x和y的变化方向是相反的，它是易增减。e增减也就说x上升的时候y大体是下降的x下降的时候呢？y大体是上升的，那这种关系呢？称为负相关。那这种是正相关。那我们关注的是线性相关关系，
也就是说呃，在这张图的第一列的两张图。这个图跟这个图好，这是线性相关关系，那么在这两个三点图当中呢，我们是能够大概观察到变量间的相关的。呃，形态和方向的，比如啊，第一张图，它的这个相关的形态是线性相关方向呢，是正相关。那么第二张图呢？它也是线性相关方向呢？是负相关，
但是呢，它不能量化。变量之间的相关强度，所以呢，我们介绍一个概念，叫做相关系数用来量化。数值型变量之间线性相关的方向和程度，那么相关系数分为总体相关系数和样本相关系数。总体相关系数。我们记为ro样本相关系数呢？我们记为r我们主要关注样本相关系数的。计算呃，这两个分别是x和y的样本方差啊，这个是我们熟悉的内容。下面这个s xy它是x和y的样本斜方差。
那这个西方它的构造呢？我们看。分子分子，它是对每一个样本点都计算一个x的粒均差，计算一个y的粒均差。然后相这两个粒均差相乘，最后求和啊，这个是分子的构造，我们看左边这张图啊，它是样本数据的散点图。垂直的虚线是x8。那也就是x的均值所在。那么，水平的虚线呢？是y8也就是y的均值所在，
这两条虚线就把平面划分为四个象限。一二。三四划分为这四个象限，那么第一个象限上的点，比如说这里。坐标是xi yi。那么xi和yi它分别都大于这个x8和y8。那这段的距离实际上就是xi和x8之间的离均差。那么，这个距离呢？就是yi和y8之间的一均差。这个yi-y八那么这条。写的是xi-x八。那么，离均差的乘积啊，
实际上就是这个矩形的。面积。也就是说，在第一个象限内的点，由于它的xi大于x8 yi大于y8，所以。这两个离君差相乘，一定是大于零的。那第三个象限呢？比如说这个点。xi yi.xi是小于x8 yi，也是小于y8的啊，也就是如果我们。计算。
这两个离君差，那这两个离君差都是小于零，那么它的乘积恰好就大于零。那么这个点的离均差相乘，实际上正好就是这个矩形的面积，那么在第三个象限。利均差相乘也是大于零的，那么第二个象限呢？比如这个点。xi yi在这个点上xi。它是小于x8的，但是yi是大于y8，所以。那么，这个离均差的。
第二个象限内离均差的乘积是小于零是负的，这个矩形的面积那么同样的在第四个象限当中，比方说这个点。xi呢是大于x8，但是yi是小于y8，所以这个离均两个离均差相乘。是负的是负的，这个矩形的。面积那这不同象限内的点的两个变量的离均差的乘积有正有负。然后我们对每个点的离君差乘积都求和啊，其实就是个正负对决的过程。它的自由度呢，是n- 1啊，除以自由度是为了排除样本量的影响。那如果这个斜方差为正。
s.xy它是大于零的，那就说明正方有优势。那正方是在一三象限，一三象限。有优势啊，大概就是这个样子啊，一三象限有优势。大概就是这个样子啊xy呢，就是正相关，如果这个斜方差为负。它小于零。那说明负方有优势，负方呢？是在二四象限，
二四象限有优势，大概就是这个样子。好二四向前也就是啊，那xy呢就呈现出。负相关的关系，所以这个斜方差本身。其实是可以表示xy线性相关的方向的啊，它可以衡量你到底是正相关还是？负相关。那它能不能量化线性相关的程度呢？答案是不能呃，因为x和y比如说这个温度x是温度。y呢，是这个病毒的存活时间。呃，
我们想观察一下这两个变量之间的相关程度的话，那如果对温度和存活时间的计量尺度发生了变化。或者使用的单位发生了变化，它都会影响这个斜方差的结果啊，它不会影响这个斜方差的符号，但是会影响斜方差的。曲池。那么，我们想同样一组样本，如果只是量纲或者计量尺度不同。那是不会影响xy的相关程度的，但是如果量纲和计量尺度不同，斜方差的值又会发生变化。那么，在这种情况下，
表明斜方差是不方便度量相关程度的。那我们为了排除量纲和计量尺度的影响，我们首先呢，对x和y都进行一个。分别进行标准化，然后用标准化以后的数值再计算，协方差就会得到。相关系数啊，标准化的操作。对xi进行标准化，那就是zx I=xi-x八。比上样本方差s样本标准差sx，那么对y呢？也是同样的操作。那么，
我们就得到了这两个标准化以后的数据。标准化以后的数据呢？按照斜方差的计算公式代进去，上面是sigma。zx I我们还要减掉标准化以后的均值，但是标准化以后的均值是零。那就是减龄就不减了，然后再整理。y标准化同样的这个y进行标完标准化之后呢，它们的均值也是零，那就是这两个相乘。比上n- 1。那这个时候zxizyi分别等于这两个嘛？我们就得到了这样一个式子啊，也就是说我们对s和y先分别进行标准化，
用标准化以后的数据计算斜方差就会得到。相关系数。那这个相关系数呢？呃，如果我们把这个sx和sy因为它跟I是没有关系的。我们可以提到外面来，提到外面，提到外面来，因为它是在分母上啊，所以我们就可以提到这个。分不上。这两个就提到下面来了，那么剩下的这个部分。呃，其实就是x和y的斜方差，
所以我们可以整理成这个样子。上面是斜方差。下面是两个标准差，就会得到相关系数，那如果我们把。这个各自的。标准差带进来，斜方差带进来就会得到。这样一个式子。这个上下啊，都是求和，然后呢？嗯，注意这个下面呢两个带根号。呃，
然后呢？这个呃n- 1都消都消掉了啊，是因为上面有一个n- 1。下面有两个根号n- 1啊，相乘还是n- 1也就消掉了，大家可以自己带进来去算一下。那这个嗯，教材上呢，用的是这种形式啊，我们整理一下，也能整理成这样。那我们先来看分子。是sigma xi-x八。yi-yi八，首先我们按照最后一个括号给它拆成两项，
那就是sigma。xix 8乘上yi再减掉sigma。xi-x八乘上y8，那么这个y8跟零加没有关系啊，它是一个常数，我们可以提到前面来。提到前面来之后，后面这是离群差之和离群差之和等于零，所以后面这一项也就等于零了。就剩下了第一项。这第一项呢？我们继续展开是sigma xi。yi减掉。sigma x 8 yi那么这个。x8它与连加也没有关系，
可以提到前面来，后面呢，剩一个sigma。yi.但是这个x八它又等于n分之一sigma xi，所以我们把x八换成这样一个式子。那就是n分之一sigma xi乘以sigma。yi那这个是分子部分啊，分母部分也是一样的处理思路。嗯，第一个根号里头这个sigma xi-x八，再乘上xi-x八啊，因为它是平方嘛，我们拆成两项相乘还是平方？然后呢，
按照第二个这个括号里头的减号，把它打开，等于sigma xi-x八。乘以xi，然后再减掉sigma xi-x八。乘以x8，但是这个x8可以拿到前面来，后面又是离均差之和，所以呢，后面这一项又等于零了。我们就把它擦掉。擦掉以后，它等于sigma xi的平方，然后再减掉。sigma x8。
xi然后这个x拔呢？可以提到零加符号来符号前面来就变成x8×sigma xi，然后我们用这个式子。呃，去替换x8那就是。n分之一sigma xi再乘以sigma xi，那这就是两个sigma xi相乘。我们又给它改成。sigma xi括号的平方。那所以。这个第一个根号里头就是这样一个形式。那同样的道理，第二项啊，那你把x换成y就好了。那就是sigma yi的平方减掉。
n分之一sigma yi括号的平方，那么这个是两个根号里面的东西，然后呢我们？再给它加上根号。这是两个相乘。那这个呢？就是分母。这是它，然后分子是它，那这个时候我们分子分母同时乘以一个n就好了。分子乘以n是n倍的sigma xi yi减掉sigma xi乘上sigma yi。那这个就是。这个部分是相等的。然后分母上乘以n，那就是说我给第一个根号乘一个根号n，
那就是。这是n倍的，那这个n就没有了，那这也是n倍的，那这个n分之一也没有了，那么就等于。这个分布部分，那这个就是教材上这个式子的来历，那现在呢？我们。这个呃，知道了相关系数的来源和计算方法，还有教材上这个公式是怎么整理来的？后面我们继续看相关系数的性质。这个相关系数的性质呢，
有五条首先来看，第一条是r的取值范围是负一到一，那如果r大于零就是正相关，如果r小于零呢，就是负相关。r的绝对值。越大表明，变量之间的相关性越强，那么当r=- 1。啊，就是这种情况r=- 1。两个变量呈现完全的负相关关系，那如果r=+1呢？就是这种。两个变量呈现完全的正相关关系呃，
如果r=0就是这样。两个变量之间是不存在线性关系的。那么，在负一到零之间，它就呈现出负相关关系在零到一之间呢？就存在。正相关关系啊，这边是负相关关系。第二个呢？是啊，具有对称性。那x和y之间的相关系数就等于y和x之间的相关系数。如果我们回过头去看公式啊，大家那个自己去看一下，如果我们把x和y互换位置，
结果是不会变的啊，这是它。呃，数学上对称性的体现，实际上我们在用相关系数研究变量，相线性相关程度的时候。并没有主次之分啊x和y。它的地位是相同的，我们研究的是它们互相之间的关系。那第三个呢？是r的大小与x和y的原点以及尺度无关啊，这个是因为我们介绍这个。相关系数来源的时候是对它进行了标准化，然后又去做了协方差，所以。
我们就排除掉了像这个圆点啊，尺度啊，量纲的影响第四个呢是r仅仅是xy之间线性关系的一个度量。不能用于描述非线性关系。那比如下面这张图xy存在的是非线性关系非常明显，它们之间的关系是曲线关系。那这个时候如果我们计算线性相关系数r啊，这个r虽然叫做相关系数，但是它是。线性相关系数，如果这个时候我们对这些点计算。这个线性相关系数r，那么这个值是零，并不能说明xy之间不存在关系，它只是不存在线性关系。
最后一条r虽然是变量之间线性关系的一个度量，但不意味着两个变量一定有因果关系。事实是这样的，我们的这个出发点，我们的目的就没奔着因果关系去啊，这是第一第二个呢。比如啊，你家门口有两棵树，它们一起长高，如果我们每周都记录一次两棵树的高度，那么这两棵树高度的变化。相关性会非常的强，但是呢，这并不能说明因果关系，这就是我们经常说的相关未必因果。
好，我们这个是相关系数的性质，下面呢，我们来看相关系数的检验。那这个相关关系的显著性检验，它是检验谁呢？它是检验总体的相关系数。柔那么用到的原料呢？是样本相关系数r。那就好比我们这个计算样本均值啊，可以作为总体均值的，估计我们计算样本相关系数也可以作为总体相关系数的估计。那我们呃用这个基于呃样本均值去检验总体均值。那也可以用样本相关系数去检验总体相关系数，那么因为样本抽样具有随机性，
所以这个样本相关系数。r也具有随机性，有自己的分布，那么就像样本均值，有自己的抽样分布是一样的道理。那我们检验总体均值的时候。用x8去检验。没有的时候。是在关于总体均值的原假设成立的条件下，也就是有一个h0。说缪等于缪零啊，在这样一个假定下。然后去确定x8的分布。然后再对x8进行标准化构造检验统计量，那构造的检验统计量或者是z统计量。
或者是t统计量那么类似的，我们要检验总体的相关系数。我们用的是样本的相关系数r。那我们也要在总体相关系数的原假设之下去确定r的抽样分布。然后对r进行某种变换来构造检验统计量。现在假如啊，我们计算的样本相关系数r。等于零点五二，那么这个时候我们可不可以？把原假设设定为row。等于零点儿五呢，那如果原假设成立，那么r它的取值呢会集中在。等于零点五附近，也就是这个虚线所在的位置。
那但是呢？由于r的取值范围，它是从负一。到一那么这个等于零点五，它的左边的活动范围就更大，右边的活动范围。就很小啊，以至于r的抽样分布会呈现出一个左偏。那大概是这个样子。那基于这样一个左偏分布去构造检验统计量是非常困难的。那么，原假设等于零点五，那是这样，那如果原假设等于。row 0，
但是这个row 0大于零，那么r的抽样分布。都会呈现出一个左偏的形态。那如果原假设是如。等于u0，但是这个u0是小于零的。那么，情况正好相反，现在这个。比如说等于负零点五啊，是这样一个假设，那么它的左边活动范围小，右边活动范围大。那么r的抽样分布就会呈现出一个右偏的形态。那不管是左偏还是右偏呃，
对我们构造检验统计上来说都很困难，那只有当我们假设。r=0啊，在这样一个圆假设之下。r的抽样分布才是对称的。那r的抽样分布，它是对称的，并且随着样本量的增加，这个分布r的这个抽样分布会趋于。正态分布，那在这种情况下才方便我们进行下一步，也就是构造检验统计量。所以呢，我们呃对这个。相关关系的检验对总体相关系数的检验就只有这一种形式啊，
就是检验总体的相关系数是否等于零。那么对应的。统计量原假设是等于零，别的假设是不等于零，对应的统计量是这样一个形式。呃，它服从t分布自由度呢是？n- 2。临界值，那就是。t2分之阿尔法啊，因为等于零不等于零，是个双侧检验当t的绝对值。大于这个t2分之阿尔法的时候，我们就拒绝。
云，假设。那至于说这个t分布为什么如此构造？在这里我们就不讲了嗯，这个跟后面的回归是有关系的。但是我们知道啊，这个检验统计量啊，就是通过样本计算来的，然后呢，根据显著性水平。去确定这样一个临界值啊，两相比较就行了。那这里呢，有一道例题吧，算是我们知道相关系数r，
我们知道样本量。n我们知道显著性水平以及对应的临界值，就可以计算这个检验统计量。t等于。零点八四三六乘以根号下。分子是n-2就是25-2，然后。除以一减阿方一减零点八四三六的。平方那么这个就是检验统计量的值。用这个值与临界值相比较呃，它大于临界值就拒绝原假设。认为x和y之间存在显著的线性相关关系。那以上呢，就是相关系数的内容，下面我们进入一元线性回归，
那我们用相关系数研究两个变量之间的线性相关程度。相关系数r衡量的这个强度，它是x和y。互相之间的呃sy的地位呢，是相同的，而且呢，我们也没有研究过呃这条直线。左边这张图呢，是代表总体的啊，我们计算了r，然后对总体的相关系数轴做了一个检验。但是我们自始至终并没有研究过，总体当中这条直线啊，现在呢，我们开始关心。
一个变量对另外一个变量的影响，比如x。对y的影响，那么此时x就称为自变量，而y呢，被称为因变量。自变量和因变量在地位上就不一样了。自变量是对因变量可以起到解释说明的作用。而因变量呢，是被解释被说明的那一个变量，那为了达到我们的目的啊，研究一个变量对另外一个变量的影响。我们现在开始关注总体当中的这条直线。呃，但是不要忘了哦，
我们呢？呃，又只能通过样本来估计这条直线，那么这条直线。到底它的。是如何的一条直线？其实我们是不知道的啊，我们也永远不会知道我们会通过右边这个样本数据。呃，样本观测点去估计这条直线。那么，为了确定一条直线，我们需要什么呢？我们需要这条直线的截距和斜率。那截距和斜率确定了，
直线就确定了，所以我们通过样本来估计这条直线。比如说啊，用样本估计的出来这样一条直线。实际上就是估计。直线的截距和斜率这两个参数。那么，我们把总体当中这条直线啊，写成贝塔零加贝塔一。乘上x。贝塔零呢是截距，贝塔一呢是斜率。那现在我们描述的是这条直线，但是总体上的各个点并不在。这条直线上，
而是围绕着直线上下波动。也就是说，如果我们给定一个确定的x。啊，比如。给定一个确定的xx 0吧，然后。对应的直线上的点是贝塔零加贝塔一×x零。但是，当x取x0的时候。y的值。是不是都在这个点上啊？不是啊，它是围绕着这个点做上下的波动。啊，
也就是说。有一个x0是确定的值，但是并没有确定的y在总体当中啊，并不是有。确定的歪的，而是这个歪围绕着这个点。在做上下的波动啊，也就是说它不是一个完全的线性关系啊，它就是一个线性相关的关系。那么，为了表达这种关系，我们在这个直线上加一个随机波动进去。也就是在。贝塔零加贝塔1x上加一个一送进去，那么这个一送呢？
是一个随机变量。啊来表示。y围绕着这条直线在波动，那么这一项呢？我们称为随机误差项。然后我们让y等于这样一个式子，那么这个式子就描述了总体当中所有点的情况。这个式子呢，被称为一元线性回归模型。所谓的一元呢？意思就是自变量只有一个呃，如果自变量有多个，那就是多元。那么这个医生。它是不是随便一个随机变量就可以呢？
呃，并不是这个随机变量要符合一定的前提假定。下面呢，我们就看一下呃。对一元线性回归模型，也就是对这个式子的。一个假定，大家注意一下，这个一元线性回归模型是针对总体而言的。我们需要对一元线性回归模型做假定，其实就是对总体的这个线性关系做假定。做假定的目的呢，一个是为了确定适用条件，另外一个是为了估计和检验。我们看关于一元线性回归模型的假定。
假定的第一条x和y之间呢，要存在线性关系。如果xy之间不是线性关系，做线性回归是不合适的，第二个呢x是固定非随机的意思就是说。x的情况对我们来说是确定的，或者我们也可以理解成我们在每一个确定的x上。去观察y的波动啊，这个时候y是随机的x是确定的啊，从这个模型当中，我们也可以看出如果。x是确定的，而且贝特林和贝特一它作为这个总体的参数。它也是确定的。好，
但是这个贝塔零和贝塔一是未知的。未知确定。x也是确定。我们后面又加了一个随机项啊，所以y是随机的，那么x我们不不认为它是随机的。后面的三四五条呃，都是针对这个呃，随机无差项一送的。首先，第三条是零均值一送的期望等于零意思，就是说对于任何一个特定的x。艺术的期望呢，都等于零。那么，
我们根据这样一个特点，对这个模型两边同时取希望。那就是。y的期望等于。贝塔零加贝塔1x再加一送的期望，然后和的期望等于期望的和。贝塔零的期望贝塔零，它是一个常数，那它自然等于本身。那贝塔一也是个常数，贝塔零是确定的，所以也等于它本身那后面。一生的期望。呃，我们假定它是等于零的，
所以后面就没了，得到一个这样的式子。那这个式子在说什么呢？就是虽然对于一个给定的x。给定的x。这个y并不是确定的啊，并不是确定的，但是对于一个给定的x。歪的期望。却是在这样一条直线上。那它是这个意思，所以这个式子其实它描述的就是总体当中的这条直线。下面呢，是同方差，也就是说不管x取值如何约数的方差是一样的。
那么，不管x取值如何y的方差也是一样的啊？因为我们计算y的方差dy=d bet a二零再加beta 1。x再加因数好。前面这一项贝塔零贝塔一是确定未知的x是确定非随机的，所以这一项它是不存在方差的。啊，它的方差是零，所以我们拆开之后其实就是最后就只剩下。一送的方差那么y的方差等于一送的方差，我们假定一送的方差等于sigma方那么y的方差也等于。是一个马坊。啊，因为我们也可以想一下啊，就对于一个特定的xy的随机性的来源，
就只有一送。呃，所以这个y的波动性与一送的波动性一定是一样的。第五条呢，是正态性啊，对于任何一个x一个特定的x，不管是三也好，四也好，五也好。伊宋总是服从同样一个正态分布。那么之前我们还知道它的既望是零方差是sigma方，现在又知道它是正态分布，所以它服从的是。均值为零，方差是sigma方的正态分布。
那这样也会导致y服从正态分布啊，因为。我们看这个式子。嗯，它的前面实际上是不具有随机性的，然后再加一个正在分布。那就是正态分布的线性组合啊，它一定也是正态分布。那我们又知道了y的均值是它y的方差是sigma方，所以y服从的正态分布的参数。均值是贝塔零加贝塔一×x。方差呢是sigma方。也就是说呢，对于任何一个确定的xi那么yi，它的分布是正态分布其中。
均值是贝塔零加贝塔一×xi，然后方差是sigma方。那这是关于模型的假定，那我们的目的是估计中间这条直线，那么直线如何表示呢？我们刚才说过了，实际上直线的表示方法就是它。呃，虽然说y是围绕着这个这条直线波动的，但是他们的期望值都在这一条。直线上那么这个这个方程呢？它是描述直线的，我们称它为一元回归方程。呃，回归方程，
它是描述直线的，我们刚才说的。回归模型，它其实是描述点的嗯，方程是描述这条直线的。那么，这条直线就简单多了。贝塔零是直线在y轴上的截距啊，贝塔一呢是直线的斜率。表示x每变动一个单位y的平均变动值，大家注意这个贝塔一。代表的是x每变动一个单位y的平均变动值，它为什么是平均变动值呢？比如说现在。呃，
我们对于一个x0。那么这个。y0。的期望等于贝塔零再加贝塔1x零。现在呢，我们让x1啊，等于x0+1，那这个时候。e.x1呃y1。等于贝塔零再加贝塔一×x一啊x1呢？是x0再加一。那现在我们看一下这个一y一减一y零。它等于贝塔零。加贝塔1x零再加贝塔一就是这个式子，
然后减掉。它贝塔零减贝塔1x零是不是这两个项都被减掉了？呃，所以这个。x变成一个单位。y的平均变动。是贝塔一。那贝塔零呢是截距啊，就是x=0的时候。在这个位置上。那么，这条直线与y轴的相交的这个点，这个点就叫做。维他林啊，这是洁具。
那这是我们关于总体回归模型和回归方程的介绍，那我们的目标呢？是要估计。这个方程啊，也就是说要估计这条直线，那就是要估计这两个参数。贝塔零和贝塔一，那我们使用的样本呢？我们使用的原料呢？是样本观察点。那我们要利用样本观察值得到这样一条。直线就是要得到这两个参数的估计值。beta 0 hat和beta 1 hat那么这个等式。就称为估计的回归方程，那么有了这个方程以后呢？
对于一个确定的x。那就会有一个y的估计值产生，那对应的这条直线上，我给一个特定的x。那么就会有一个与x对应的y的估计值出现。那我们下面就是要想办法来，估计这两个。参数也就是估计这条直线的截距和斜率。那我们看。呃，我们先不管啊，这条直线我们是如何去得到也先不管得到的，这个直线它的。呃，截距是几呃，
斜斜率是几啊那？这个样本观测点，它总是围绕在这个直线的周围嘛，那对于任何一个x一个点啊，哎，它对应的坐标xyi。x.比如说呃，这个点它的坐标xi。yi那么其中yi。啊，是这个点纵坐标的一个真实值，它对应的估计值在哪里呢？在这里。呃，
这是估计值，这是yi hat啊。估计是在这里。那么，真实值与估计值之间的差。有了这段距离。被称为残场。呃，这个是残差。那么，残差是有正有负的啊，这里我们现在展示的这个残差，它是正的。那如果是这个点呢？这个点。
的yi是在它估计值的下方，那么我们用真实值减去估计值，那一定会得到一个。负的残差啊，所以残差有正有负，那我们就用残差的平方啊来表示每个点与其估计值之间的这个波动，或者说是距离。如果我们的样本量是n。那就有n个残差，也会有n个残差的平方，那么这n个残差的平方。平方和平方的和啊。就是它，我们称为残差平方和。那么呃，
我们画这样一条直线。那么样，因为样本是确定的嘛，我们画这样一条直线就会产生一个残差平方和，那如果我们在。做另一条直线，它就会产生另外一个残差平光和。那么，在样本确定的情况下，直线变化，残差平光和。就会变化。而直线变化就是这两个参数的估计值在变化，那在我们可能得到的直线中，有千千万万无穷无尽的直线。
从残差平方和的角度。我们肯定是希望得到那条能够使残差平方和最小的直线。因为残差并方平方和，最小意味着说啊，我现在得到的这条直线啊，能够使得。点围绕着直线。达到一个最紧密的程度，那么我们要使得残差平方和最小就等价于要找到使得残差平方和最小的这两个。估计值呃，这是残渣平方和的一个。表达式。那xi yi都是样本观察结果。那这个是已知的情况了，就好比我们把这个观察结果已经画在这个图上了，
这些点我们都描在图上了，它是不会动的了。影响残差平方和的是这条直线，那么这条直线动意味着这两个参数的估计值在动啊，所以现在这个残差平方和。是这两个估计值的函数啊，而且是个二元函数，那这个平方和它没有最大值。但是它有最小值。呃，就好比这个二次曲线啊，它没有最高点，但是它有最低点。呃，最小值在这个导数等于零处取得，
所以呢，我们现在。呃，分别对。贝塔0 hat和贝塔1 hat这两个参数求偏导，并且让这两个偏导都等于零。就会得到两个关于beta 0 hat和beta 1 hat的方程，构成一个方程组呃，两个方程，两个未知数。那我们就可以这个通过解方程求解这两个未知数，使得啊，我们解出来的这两个参数。构造的这个直线达到残差平方和最小。我们来看一下这个导数是具体是怎么计算的？
那这个是参差平方和的表达式，我们要分别对贝塔零和贝塔一呃，贝塔0 hat和贝塔1 hat求导，并且让。导数等于零。那么，首先我们对别的零修到呃，这还是一个复合函数，而且带连加符号。那么连加符号，你可以先不管它呃，因为这个和的导数呢，等于导数的和，所以连加符号，我们先不管，
我们先对后面。的求导，它是复合函数，我们先对整体求导，对整体求导就是两倍的括号。yi-beta 0 hat，减去beta 1 hat xi，然后再乘以。对括号里边的这个式子呢？关于贝塔零求导，那么关于贝塔零求导，这个yi它是个常数。呃对贝塔零求导是零，那后面这一项呢？xi是常数贝塔一在这个时候呢，
我们也看成常数。那求对贝塔零求导呢？也是零。就剩下了这个部分，那这个部分对被特灵求导，被特灵害的求导是负一，也就是乘以一个负一。呃，那就还要加上一个连加符号，它等于零，那么这个二和这个负一提出来就是负二，当它等于零就得到了。第一个式子，那么第二个式子。首先还是对。
整体求导就是它，然后。再对括号里的这个式子呢？关于贝塔一求导，那么同样的道理呃，这个导关于贝塔一派的求导是零，这个关于贝塔一派的求导是零。那么，后面这一项对贝塔1h的求导是个负的xi。然后把连加符号加起来。负二负号，还有二提到外面来就得到了第二个式子。那这个是关于。求导数，那我们就得到了两个方程，
然后去解这两个未知数，我们看第一个方程。因为它整体是零，所以我们把负二约掉以后呢，后面这个连加呢，一定还是零，所以我们只看后面这个连加就行，那么第二个式子也是。只看后面这个连加，首先来看第一个连加。它是sigma yi减掉sigma beta 0 hat，减掉sigma beta 1 hat。xi=0，那么这个beta 0 hat是跟这个I没有关系的，所以这里就直接写成。
n倍的贝塔0 hat。这个beta 1 hat呢，也是一样的，它可以提到外面来。这就是sigma yi减掉n倍的beta 0 hat。减掉beta 1 hat sigma xi=0，然后下一步我们把。yi连加啊，写成n倍的y的均值，减掉n倍的贝塔0 hat。剪掉。beta 1 hat那么sigma xx I呢？我们也写成n倍的x吧。它等于零，那这个时候呢n我们可以都消掉啊，
因为n不等于零，只能是剩下的这个部分等于零。啊，这个时候beta 0 hat就等于y8减掉beta 1 hat。乘上x8好，那么就得到了这个贝塔零和贝塔一之间的一个。关系。而且这个关系表明我们估计的那条直线一定会通过一个点，就是x8。y8。这也是啊，这个直线的一个很重要的特征就是它一定会通过这两个均值组成的这个点。我们再来看这个beta 1 hat是如何得到的。看第二个式子。呃，
也是把它展开sigma。xi yi减去。sigma xi beta 0 at。减去sigma xi beta 1 at。xi.等于零。然后呢？beta 0 hat提到，前面来beta 1 hat提到，前面来。等于。sigma xi yi-beta 0 hat。sigma xi-beta 1 hat sigma xi的平方啊，这两个xi相乘。等于零。
再把贝塔零呢，用这个形式替换掉。是sigma xi yi减掉括号y8-beta一。hat x8×sigma xi。减掉beta 1 hat sigma xi的平方=0。接着。这个是sigma xi yi减掉y8乘上sigma xi。然后再加上beta 1 hat x8乘上sigma xi。然后减掉beta 1 hat sigma xi的平方。等于零下一步我们再把这个x8y8啊。不要用这种形式了啊。用这个n分之一sigma的形式再会替换回去，等于sigma xi yi减去。n分之一sigma yi乘上sigma xi。再加上。
beta一hat n分之一sigma xi，然后还要乘上一个sigma xi，那就直接是它的平方。减掉beta 1 hat sigma xi的平方=0，那我们发现。这个。前面这两项啊，它跟这个beta 1 hat都没有关系，后面这两项是有关系的，我们后面这两项呢，我们给它合并成一项。那就等于再加一个beta 1 hat括号。n分之一sigma xi。的平方，然后再减去sigma xi的平方。
括号等于零。那这个呢？就照抄过来。sigma xi sigma yi，我们加起来等于零。那这样再把这个beta in hat解出来。啊，就方便多了啊。我们把前面的呢都擦掉。进一步整理一下，那就是。sigma xi yi减去。n分之一sigma xi乘上sigma yi。等于。beta 1 height。
这个括号里呢？就变成了sigma xi的平方，减去n分之一sigma xi括号的。平方那么beta 1 hat就等于。左边这一项除以它。sigma xi yi减去n分之一，sigma xi乘上sigma yi。然后比上。sigma xi的平方减掉n分之一sigma xi括号的平方。然后分子分母同时乘以n。那么，这边乘以n这边乘以n就是一就没有了，那么这边乘以n这边也乘以n那么就这个也就没有了。最后呢，就是这个形式，
然后对比一下，看是不是？n sigma xi yi-sigma x sigma y比上n倍的。平方啊，它是一样的，那这个贝塔一呢就解出来了。那这个beta 1呢？这种方式是我们教材上提供的，那实际上呢？我们也可以用下面这个。呃，方式，而你下面这个方式呢，其实也更容易记忆一些。那这种方式，
它的分母和分子是斜方差的分子，那么分母上呢？是x方差的分子啊。呃，关于这个后面这种方式的证明呢？我们就不用直接的形式了，我给大家一个提示啊。还是分子分母？同时关于后面这个减号展开，我们下面这个平方呢，就给它写成啊，平方不要了，再把这个xi-x八。写一遍，写一遍，
以后呢，我们展开之后，后面这一项它就等于零，下面呢，也是等于零，这个我们再讲。相关系数的公式的时候。是演示过这种操作方式的。那么，分子上只剩这一项，然后再按这个减号展开，那分母上只剩这一项。也按这个减号展开，最后呢，也会得到。
这样一个形式啊，这两个beta 1 hat啊，本质上它是相等的。那下面我们看一下，假设我们现在估计出来一个方程就是这个样子，那么这个是beta 0 height，这个是beta 1 height。它们的含义是什么呢？说首先我们先来看这个贝塔0 hat，贝塔0 hat呢，它其实是截距项表示x。等于零的时候，这个yh at是多大？但是这个呃beta 0 hat呢，我们对它并没有怎么关注啊，
因为第一点这个线性关系的体现其实是主要体现在斜率上啊。第二点呢，这个洁具有很多时候它是不能对现实的意义，做一个合理的解释的，有的时候甚至这个解释会很荒谬。啊，比如如果说我们根据样本计算出来的啊，这估计出来的这条直线。是这个样子，比如这个是身高。啊，这个是体重。那么洁具呢？它就会告诉你当。x=0，
也就是身高等于零的时候y呢，体重是个负数啊，这种这种解释其实很荒谬。那具体到很多呃，实际应用当中都会出现这种情况，所以这个贝塔林啊，你知道它是。x取零的时候在y轴上的截距就可以了啊。关键呢，是对这个beta 1 hat的解释，这个解释是这样的呃。x.美。增加。一个单位。
y平均。增加。beta 1 hat格。单位。那具体到我们这个是当中啊x每增加一个单位y就增加零点零三七八九五个单位。那如果x每减少一个单位呢？y平均就减少零点零三七八九五个单位，那x和y它的变动方向是一致的。什么叫这个叫每增加一个单位呢？就是说我不管你x从哪个值上增加你，比方说你x本来等于。等于二你从二上增加一个单位y的平均增加是贝塔一个单位。那么，你如果x从四上增加一个单位，那y呢？
平均增加呢？还是贝塔一个单位啊？这叫每增加一个单位？y的平均的增加呢，是一样的啊，因为它总是沿着这个斜率走啊，所以这边。你变动一个单位y的增加。跟在这边x增加一个单位y的增加，它是一样的。那这是关于系数的解释。那我们使用参差平方和呃最小求截距和斜率估计值的方法呢，就叫做最小二乘法，现在呢，我们已经知道了它的。
求解过程呃，接接下来呢，我们来看一下，对我们求解的这个方程的一种评价啊，就是拟合优度。首先来看判定系数。那左边这张图呢，有两条线黑色的，这条线就是我们的。估计的方程。那么呃，蓝色的虚线呢？它是蓝色的，这个水平的虚线呢？它是y的均值。
外拔啊。呃，我们用y的离均差平方和呢，可以衡量y的总波动，也就是说我们只关注这些点的。纵坐标，我们看它纵坐标的总波动的时候呢，我们可以用这个离均差的平方和来衡量。呃，但是呢，这每个离均差又可以分为两个部分啊，比如这个点是我们真实的一个观察值，它的坐标。是xi。yi.
当然了，我们现在只关心yi那么均值呢，是在这里。我们可以计算一个离均差。这是yi-y八。那这个离均差呢，你看它穿过了这条直线。那么，从真实值到直线上这一点，这一点实际上是yi。hat啊，是yi的一个估计值。那就是。yi-yi hat是这一段，那么下面还有一段是这个部分。
那这个部分呢？就是yi hat-y八。那么，既然这个离君差，我们可以这样分解成两个部分。那么，做这个离均差平方和呢？我们也可以把里头这个离均差按这样的方式进行一个。分解组合，然后把平方打开，那么打开平方的时候，这一项的平方，这一项的平方。那么按理说呢，还会有一个两项的交乘项，
然后乘以二，但是交乘项等于零，那么在这个总的平方和的分解中呢？交成像就消失了，就剩下两个平方和啊，这也是总平方和的分解，我们在方差分析当中呢，就已经见到过总平方和的分解了。那么现在呢，又是一种纵平方和的分解，那么。这个分解呢，其实是跟方差分析当中的总平方和非常的相似，那我们看。我们找的这个点。
它脱离了均值是这么远。那这个距离是yi-y八，那么它之所以能够脱离均值这么远？是为什么呢？有一部分就是x可以解释的是哪一部分呢？是这一部分。就这一部分，那么这一部分就是x它在这个位置。嗯，所以引起的它的估计值能够到这个位置，这个这个位置是yh at减。y8那这个部分是x可以解释的，那么这个部分呢？它是真实的值减去估计值这个部分就是x没有办法解释的x只能解释。你为什么从均值到了这个位置？
而它不能解释，你为什么从估计值又跑出去这么远啊？这个就是x不能解释的了，那么x可以解释的部分。是这个部分，我们称为ssr，是回归平方和那么这个部分是x不能解释的是ss e就是剩余平方和其实就是残差平方和。那么，总的平方和呢？我们还是记为s ST，那么在这两两个平方和当中回归平方和也就是。这个部分的平方，然后求和是x可以解释的，剩下的呢是x不能解释的。那我们用x可以解释的这部分平方和比上总的平方和就是x可以解释的那一部分的比例。
称为r方，又称为判定系数，那么根据残和三个平方和的关系，这个判定系数r方又可以写成这种形式。r方的取值呢？是零到一啊，它的值越大，说明x能够解释的总波动占比就越大。拟合优度呢？就越好。当然了，我们也可以把这个平方和呢。写呃，把这个ss安娜写写成平方格的形式s ST写成平方格的形式，它就是这样一种形式啊。
下面呢，我们来看一下这个教程项目，它为什么？等于零。那根据我们这个呃，两个求偏偏导的式子，那这个。负二我们就不管了，因为负二消掉之后，后面的连加呢，照样等于零。那这个后面连加符号，括号里边是什么呢？就是yi-y。I hat.
那就是。三叉。所以说呢，实际上残差的和是等于零的第二项。括号里边儿依然是残差，所以这个xi乘以残差再求和也等于零的。好，这个是我们通过求偏导推出来的，两个等于零，然后这个交乘项。的第一项实际上又就是残差ei，所以我们整理成这个形式，然后呢？按这个括号展开，第一项是sigma 1I乘上yi hat，
第二项是ei乘上y8。那个外拔呢，我们可以提到前边来。后面就剩下谁了，残差的和等于零，所以第二项它是等于零就没了，我们来看第一项。第一项当中，这个yi hat我们写成这种形式，然后再按这个加号给它展开。第一项是残差乘上bit 0 height，那么这个bit 0 height是可以提到外面来，后面又剩下残差的和等于零。那这一项呢？贝塔一可以提到外面来，
剩下了一个1I和xi乘积的，和它也等于零。所以那么这一项呢，也就等于零了，这就是交乘信号，为什么等于零，这就是为什么总平方格可以分解成这两个平方格的原因。下面呢，我们看另外一种评价方式，就是估计标准误差，那么从总体的回归方程当中。我们可以得到这个随机误差一送的这样一个表达方式，那么从呃样本的这个估计方程当中，我们可以。得到残差的这样一种表达方式，
那么随机误差的方差是sigma方。但是我们在样本当中是得不到随机误差的，我们能够得到的。是残差，那现在我们要估计随机误差的方差就可以使用残。含差的样本方差来做这样一个估计。那具体要怎么做呢？残差的总波动除以自由度就是残差的样本方差，那么残差的总波动。其实就是ss e就是剩余的平方和残差的主波动。等于它，那就等于这个式子嘛。那分子上是ss e注意自由度就得到参差的样本方差，那么这个自由度是n- 2。这个有两种理解方式，
一种理解方式呢，是在一元回归当中，我们要估计两个参数。所以就消耗到掉了两个自由度，所以自由度是n- 2。那么第二种呢？就是说呃，在一元回归当中，自变量的个数是一个。那么，这个n- 2实际上是n减去自变量的个数k再减掉一那么一元，回归当中只有一个自变量。导致这个自由度呢？是n- 2啊，这是两种理解方式。
教材上应该是这种啊，它把k看成是自变量的个数，然后这个自由度是n-k- 1。就是n- 2了，那么这个m se其实就是。军方。啊，剩余平方和的。军方那么这个军方m se或者是。这个残差的样本方差作为sigma方的一个估计，然后我们对它开根号就。作为sigma的一个估计，那么当然了，我们做完这个方程的，估计以后我们肯定是希望这个。
含插的。平方和它越小越好啊。m se或者根号mmse，它越小，说明这个我们的样本观测点与直线围绕的。越紧密就说明你和优度越好。但是这个单纯的计算m se啊，或者根号下m se本身的意义其实并不大。呃，但是呢，这个嗯，对我们后续做检验是很有帮助的啊，这个m se。那这个是两种关于估计的方程的评价，一种呢是r方。
判定系数啊，判定系数是零到一，然后它的取值是越大越好，那么第二种呢？是估计标准误差是这个se。呃，它当然是越小越好呃，下面呢，我们就看这个，我们对方程的两种检验。第一种呢，是线性关系检验，这个检验呢，跟方差分析的检验呢，也很相似，
它的出发点呢，是从全局从整体考虑问题。那能够用自变量解释的部分的均方。呃与不能用自变量解释的部分的均方做比值，就会得到一个f统计量，这个与方差分析分析很相似。那么，方差分析的原假设呢？是k个总体均值是相等的，对应到回归方程的线性关系检验，原假设就是方程中。的斜率全都等于零。啊全等等于零，因为这个线性关系的体现呢，就是斜率，
而不是截距斜率，如果等于零，就意味着变量之间不存在线性关系。但是在一元回归当中呢，就只有一个斜率贝塔一啊，所以原假设就是贝塔一=0。我们在贝塔一=0的情况下，构造两个均方的比值，然后服从f分布。那么，这个军方呢？是平方和，然后除以自由度得到的，那么这个自由度还是两种理解方式，如果我们把k。
理解成参数的个数。那么上面就是。k- 1下面就是n-k，那因为参数个数是两个，那如果我们把k理解成自变量的个数。一元回归当中自变量只有一个，那么这个时候这个自由度就是k这个自由度就是n-k- 1。呃，书上应该是用这种方式啊，可以表示自变量的个数。那这个呢？你如果说单纯的想理解这个问题，记忆这个问题，你用哪种方法都可以，但是呢，
你不要记混了，你不要上面用参数，下面用自变量。你也不要上面用自变量，下面用参数啊，你只你如果记的话，你只按一种去记k，要么就都是上下都是参数，要么上下都是自变量的个数。那么，这个是自由度的问题。呃，具体的这个检验呢？它的步骤啊，这个就我们就很轻松了，
首先提出假设，原假设呢是。全部的斜率等于零，但是一元回归当中只有一个斜率，那就是贝塔一=0，然后计算f统计量。是两个军方之比，那么这个军方来自x可以解释的部分，这个军方来自x不可以解释的部分。同样还是右侧检验临界值呢，是t了。右侧检验这个跟方差分析当中的f检验是一样的了，一样一样的道理。因为如果这个呃原假设。不成立。
那一旦不成立这个呃，回归平方和。ssr.会变大。啊，所以它还是一个右侧检验呃，但是你还是要呃，记住一定要把这个msr放到。前面放到这个分子上，才是右侧检验，那么临界值呢？呃。我们从右边划走阿尔法对应的这个分位数，就是f阿尔法。如果我们计算得到的f统计量比这个临界值要大。
我们就拒绝原假设表明啊，变量之间是存在显著的线性相关关系的。下面我们看第二种检验回归系数的检验，那我们要学习的呢？还是关于这个斜率系数贝塔一？它是不是等于零的这样一个检验？也就是说，原假设还是beta 1=0，那么这个beta 1=0，这个原假设这不跟这个线性关系检验是一样的吗？呃嗯，这个在学一元回归的时候，大家可能还是有点分不清楚呃线性关系的检验呢，它是从整体出发的。啊它呃，
但是在一元回归当中呢，系数就只有一个，它只能写成这个样子。那么，在回归系数检验当中呢？它其实是一个系数，一个系数的检验。那么，在一元回归当中，又只有一个系数，那所以只能检验贝塔一=0。呃，这样一个原假设，那么如果是多元的话呢？在多元回归当中，
它有x1x2等等呃多个自变量的时候，那么关于线性关系的检验就是beta 1=beta二。等于贝塔k=0啊，这是关于线性检验的原假设，那么关于系数的检验呢？是分开的。贝塔一啊等于零原，假设贝塔二=0啊，这样一个原假设一直到贝塔k。等于零一个原假设它是分开，每个系数都检检验一遍，但是我们一元的那就线性关系的检验就只有贝塔一那，所以就是贝塔一=0原假设。那么系数呢？又只有贝塔一，
所以后面也没有，就只有贝塔一=0啊。所以这个在一元回归当中啊，线性关系的检验和系数的检验，原假设是一样的，只不过出发点不一样，刚才是我们是从总总体。出发看看x能够解释的部分和呃。它不能解释的部分的均方的比值是如何的，现在呢？我们就不是从整体出发了，我们就是单独看啊beta 1。是不是？等于零啊，这样一个假设，
那么这种检验呢，其实跟我们之前学过的。这个假设检验当中第八章啊，是很相似的，比如我们检验缪啊，关于缪的一个假设的时候。那我们是怎么办的呢？我们是用缪的，估计值也就是x8啊。从x8的分布开始。去构造检验统计量等等等，然后呢？我现在我们估计这个贝塔一啊，检验这个贝塔一啊，是不是等于零？
我们也要从beta 1的估计值出发，也就是beta 1 hat出发。那去构造检验统计量。那beta 1 hat它其实是一个随机变量啊，因为这个呃beta 0 hat和beta 1 hat最后计算的结果是取决于样本的样本，又有随机性，所以这个估计值。特灵hat和贝特1 hat。它都是随机变量，它有自己的分布，我们首先就要弄清楚这个估计量的分布，就像我们当初要弄清楚rx 8的分布。才能构造检验统计量是一样的，那现在我们看。这个是beta 1 hat的公式啊，
然后我们怎么处理它呢？我们还是按照后面这个括号展开。展开之后，第二项等于零就剩下了这一项。那么，我们可以观察到。这个beta 1 hat。它的随机性来源于yi啊y，确实也是随机的，那么前面关于x的这个部分。其实它都没有随机性啊，所以beta 1 hat的随机性来源于yi，那么我们就要研究一下。yi是怎么样的？那么yi我们在介绍这个回归模型的时候啊，
基于对。随机误差一送的，假设我们知道yi本身是服从正态分布，这个是它的期望和方差。那么beta 1 hat啊，又可以看成是yi的线性组合。所以beta 1 hat呢，也是服从正态分布。现在呢，我们确定了beta 1 hat服从正态分布，然后我们看一下它的期望和方差，那么如果我们能够确定了。beta 1 hat的基本和方差，那么beta 1 hat服从的分布就完全确定了。我们看一下beta eha t的数学期望。
我们刚才说过，说到这个贝塔一，它的随机性完全来源于这个yi。所以我们求贝塔一的数学期望的时候，其他地方不用动啊，把yi换成它的数学期望就可以了，也就是说把这个。视觉期望换进来就会得到。这个式子，那这个就是的数学期望，然后呢，我们按照后面这个加号给它展开。展开以后。这个贝塔零我们可以提到连加符号，外面后面是个离君差之和等于零，
所以第一项就等于零了。那么第二项呢？是。这样一个形式，贝塔一。啊，这样可以提到前面来。贝塔一放到这儿，但是后面呢？这个就不它就不是离均离均差之和了啊，它是离均差乘上xi再求和。它就不等于零了。我们看分母分母呢，我们写成一个xi-x八，再乘上一个xi-x八的形式。
按照这个括号的减号展开第二项等于零，那么就只剩下这一项，那我们发现我们。分子上这个贝塔一提到外面来的时候。剩下的部分跟分母是一样的。跟分母是一样的，那这个地方它等于零没有了，那这个部分跟分母一样约掉了，就只剩一个贝塔一啊，所以贝塔1 hat的数学期望就等于贝塔一。我们再来看这个beta 1 hat的方差。那么现在呢？我们就还是用呃这个形式来计算它的方差呃分母上是。样本。数据xi的总的波动，
它其实是一个常数，我们可以把它提出来，它在分母上提出来。要加一个平方啊。分母就处理完了，然后呢？求分子的。方差那么分子的方差呢？它其实是一个和的方差的形式，那么和的方差呢？等于方差的和啊。那对于每一个xi比方说x1-x八。括号y1那这个时候。我们对它求方差。它实际上等于谁呢？
等于x1-x八，因为对于一个对于x1-x八这个式子来说，它是一个常数，所以提出来。加个平方，后面剩一个y1的方差那么y1，它的方差是谁呢？sigma方啊？就是y1它它其实不是一个呃确定的值啊，它只是说。这个x取x1的时候那么y在哪里？那个y呢？是个随机变量，它的方差。是sigma方，
所以每一项都是这样的，加总起来就是这样一个形式。这样一个形式，那现在。分子分母是可以约分的。像sigma平方啊，因为它可以提到这个外面去啊，提到外面去分子就是sigma平方。sigma，然后离君差的平方，然后分母是谁呢？分母是这个离君。x平方和的平方。啊，所以这边约掉这边把平方去掉就可以了啊，
就得到一个这样的式子。那么这个式子我们。再整理一下就可以得到这样一个情况了，那么得到这这个是怎么来的？我们之前应该这种方式已经讲过很多遍了哈。就是把sigma把下面这个平方的形式写成sigma xi-x八。乘上xi-x八。然后按这个减号展开，后面展展开以后呢，后面一项等于零，就只剩下sigma xi-x八×xi。然后再按它展开就可以了，就可以得到一个这样的形式，那么这个是我们教材上给的形式。那到目前为止呢，
我们知道了beta 1 hat服从正态分布，并且知道了beta 1 hat的期望和方差。现在呢，我们把bit in hat。不同的分布。表示出来那么beta in hat服从这样一个正态分布，那么sigma。呃beta 1 hat的平方其实就是beta 1 hat的方差了，这个是我们刚刚这个已经知道的，那么既然它浮动正态分布，我们对它进行标准化，减掉它的期望，除上它的标准差。是服从正态分布的，但是这个正态分布呢？
是不能用的，因为这个。beta 1 hat的方差当中包含一个。呃，随机变，随机误差项一中的方差，那这个方差我们是不知道的，但是我们可以用估计的标准方差啊去。代替它啊。去代替它，这样就实现了我们用。beta 1 hat的样本方差去代替它的总体方差的这样一个局面那。所以呢，我们在构造这个统计量的时候用。s.
beta 1 hat去代替这个sigma beta 1 height，也就是说啊。这个地方。这个地方。换成了这个值。那他就服从了。不是正态分布，而是t分布。t分布的自由度呢？是n- 2，这个n- 2。它的来源是这个se方啊，也就是ss e。它的自由度啊，是n- 2。
那既然我们已经构造了检验统计量，下面就容易了，第一步提出假设原假设是系数等于零，别的假设是不等于零。如果系数等于零的话，那么这个检验统计量的这个部分就没有了。嗯，就只剩下beta eha t，比上beta eha t的样本标准差。样本标准差，我们可以通过这个嗯，我们得到的样本计算出这个t统计量的值啊。那么，关于等于零不等于零的这样一个假定是双侧检验，那如果计算出来的这个检验统计量的值的绝对值大于这个临界值，
就拒绝h0。拒绝h0认为。这个系数是显著不为零的好。那么下面呢？我们这边有几个数据啊，实际就让我们熟悉一下这个呃检验统计量如何计算，如何去做决策？那么t- 1统计量等于beta 1 hat嗯，是这个零点儿零三七八九五。比上它的标准差是零点零零五零三零。那么临界值。t.呃，二分之阿尔法等于二点零六八七嗯，那这个值啊，
应该是大于二的。那么，这个就拒绝原假设啊，认为这个系数显著不为零。那这个是呃，两种检验，一个是线性关系检验，一个是回归系数的检验。那如果我们估计得到的方程通过了线性关系检验，通过了系数的显著性检验。表明这个系数显著不不为零，并且嗯，变量x和y之间存在这个显著的线性关系。那我们就可以用我们估计的方程啊，做预测那么所谓的预测呢，
就是呃，给我们一个特定的x的值，然后对y进行点，估计和区间估计。但是这个估计呢，要分情况，一种情况是对y的均值的估计，一种是对个别值的估计。那假如左边啊，是总体和总体回归方程啊，右边呢，是样本数据。那我们通过样本数据估计的方程啊，是比如说就是这条线。现在如果给我们。
x=2。让我们对总体均值做点，估计那应该怎么做呢？那我们当x=2的时候，总体的均值啊，应该在这个位置。那么，在x=2的时候，我们的估计值应该在这个位置。那么，这两个位置的y的坐标。一定是一样的吗？不一定啊，不一定这个就跟x8不一定等于MU是一个道理。那我们做点估计怎么做呢？
我们就是直接用直接把x=2带到。这个估计的方程当中。得到一个y的估计值，作为对这二个总体均值的估计。啊，注意，我们现在估计的是这个点，那我们就用这个点的YY值去，估计这个点的y值。呃，这个就是对总体均值的估计。那另外一种呢？呃，就是呃，对y的个别值做点估计那什么意思呢？
那之前估计x=2的总体均值意思就是说。在总体当中x=2的个体有很多个。那么，这很多个我们可以把它看成他们，他们形成了一个小的总体呃，什么张三李四王五小米小帅等等等，他们的x都等于二。我们只关心它们的均值，对均值做估计，那就是对这个点做估计。估计的方式就是我们直接把x=2代的估计方程当中会得到一个y的估计值，作为对这个总体均值的。一个估计就像我们从呃抽样得到样本均值，直接作为总体均值MU的估计值是一样的，那么现在这个呃对于个别值的估计的意思是什么呢？
就是说这里面。我现在关心的不是这个小总体的均值了，我现在只关心其中的一个，比如说关心这个。呃，小美的y值，那我们如何去对小美的y值做点估计呢？小美的y啊，它其实是围绕着这个点在波动的。这就相当于在这个点上面加了一个随机误差，那么因为是随机误差呢，我们也不知道具体在哪。但是这个随机误差的期望值是零啊，所以我们还是把x=2。带到这个估计方程当中，
又出来一个y的估计值，作为对小美的y的估计值。啊，所以在对y的点估计当中啊点估计当中，不管是对均值的点估计还是对其中某一个点的估计。个别值的估计都是同样的处理，就给定一个x=x零，直接把x0带到这个方程，得到y0 hat。它同时呃，作为均值的估计和y的个别值的估计啊，也就是说。这个总体的均值的估计，或者你只关心小美，或者你只关心小帅，
他们在x=2时y等于几？都是一种处理方式啊，就是把x0带到估计的方程的中，当中得一个y作为对均值的估计，同时作为对个别值的估计。下面我们看这个区间，估计区间估计就比较复杂，我们要知道y0 hat它的分布才行。那就好比这个，我们对总体均值做区间估计对缪做区间估计。要确定x拔的分布啊，是一样的，因为我们知道了x拔的分布才能进一步构造数轴量啊，首先我们来看。对均值的区间估计。
那我们要知道YY I的分布是正态y8的分布也是正态。我们对yh at呃做一个整理yh at本身是这样一个形式，我们用。beta 0 hat=y八减掉beta 1 hat x8。啊，用这个部分替换掉beta 0 hat就得一个这样的形式，那么后面两项呢？与beta 1 hat有关，我们把后面两项合并成一项，那beta 0 height就是一个这样的形式了。那么y8是正态分布啊y1 hat呃，这个beta 1 hat我们刚刚啊也解释过，也是正态分布，所以它又是正态分布的线性组合。那么，
贝塔0 hat呢？也是正在分布。我们看一看这个正态分布，它的期望和方差。那y0 hat它的期望我们已经啊。就把value hat整理成这个形式了，我们就按这个形式去求它的期望就可以了。呃y8它的期望是这个部分。带进来。然后。这个部分的期望，因为对于一个特定的x0来说，这是个常数啊，所以它的期望等于它本身。beta 1 hat的期望呢？
又等于beta 1啊，所以是一个这样的形式，这样形式之后呢？把这个括号打开，这边有一个beta 1x8。这边又减掉一个贝塔EX 8，那就没有了。剩下一个贝塔零加贝塔一。x0那这个就是y0 hat的数学期望，那么它的方差呢？我们还是按照呃这个形式去求，那么差的这个方差，我们用方差的差。啊来表示，就分成了两个部分，
那么第一个部分y8的方差是它啊，就是它。那后面这个部分。x对于一个特定的x0x8-x零是一个常数，提到外面来要加一个平方。后面呢，就只剩一个beta 1 hat的方差，那么beta 1 hat的方差呢？我们之前啊，不往前翻了，我们之前已经求出来带进来就行了，带进来之后就是这样一个形式。呃，当然也可以把sigma方提出来。括号里。
是用这样一个形式，那看起来还是呃有点复杂啊。那现在我们知道了YY 0 hat，它是正态分布，并且知道了它的期望和。方差那么现在。我们要检验这个均值的呃，这个呃，我们要对这个均值做区间估计的话。那么这个地方。是谁呢？其实就是ey 0啊，我们其实就是要对它做提前估计。那么，它是相等的，
那么基于这样一个呃分布，我们对y0 hat做标准化。就会得到一个。标准状态分布，但是这个标准状态分布又有问题，它不能直接用，因为这个sigma我们是不知道的，不知道怎么办呢？用它的估计值去代替。那么，代替完以后，它不服从正标准正态分布了，开始服从t分布自由度是n- 2，那么这个自由度的来源是哪里呢？来源就是它。
所以我们对这个均值做区间，估计就是。从啊，把这个式子啊带到这个具体的分布当中啊，但是左边和右边都各切走二分之阿尔法。让它落在这样一个区间里，然后呃，通过等价变换呃，把这个期望反解出来，就会得到一个这样的区间，那么这个是。均值的执行区间，那么个别值。应该怎么去？估计或者怎么去预测呢？
那么个别值，它不是一个固定的，我们对这个。y0的期望做估计的时候，它的期望是固定的。就好比我们就好比这个地方是x0，那这个地方。是y0的期望啊，它是固定的，那么个别值。它不固定，它是围绕着这个固定的ey 0在做波动。在做波动，那么我们在预测这个个别值的时候，就要把这个波动加进去，
那就要把这个波动加进去。加怎么加呢？就加到原来这个方差当中啊，因为那个随机误差，它的波动平均波动sigma方对吧？要把sigma方加到这个方差当中。再加一个sigma方。啊，那它本身。它本身是。sigma方，然后呃，sigma方乘上n分之一，然后再加上一个呃，这个式子。
我们再加一个sigma方的话，相当于往这个括号里再加一个一。再加一个，一进去。那实际上就是这个部分。这个部分那所那所以我们在对个别值进行预测的时候就是要。在这个对均值的。区间上你要额外加一个波动进去，这个额外加一个波动，其实就是相当于往这个括号里。加了一个，一进去。那么，在嗯构造这个预测区间的时候呢，就成了一个这个样子，
那么。我们看置信区间啊，置信区间和预测区间，它俩相比啊，同样的置信水平，同样的样本。估计出来的同样的直线。那么，个别值的预测区间要比这个均值的知性区间要宽啊，因为它这里多了一个一。呃，那么对于呃置信区间也好，还是预测区间也好？它的。宽度的变化呢？
还会受到这个值的影响啊，因为其他值都是固定的，只有这个值在变化。也就是说你给定的这个x0，它离x8越远，那么这个平方就会越大。以至于这个区间就会越宽，那什么时候能达到最窄呢？就是你离x8越近，它就越窄。当这个x0就等于x8的时候，这一项就等于零了就消失了。那么同样的道理，对个别值做预测的时候，如果给定的x0恰好等于x8这个部分也就消失了。
啊，这个时候。置信区间均值的置信区间和个别值的预测区间都达到了这个最窄。那这个就是我们呃一元线性回归的内容。