# 第11章：数值型变量之间相关关系的测量
大家好，今天呢，我们就进入第11章，这是最后四章当中最重要，也是最后四章当中最难的一章。我们在第九章和第十章分别介绍了分类变量之间关系的测量、分类自变量与数值型因变量之间关系的测量。那这一章呢？我们就介绍数值型变量之间相关关系的测量。
## 一、数值型变量关系的类型
两个数值型变量之间的关系呢？我们分为两种：
### 1. 确定性的关系 (又称为 函数关系)
这种关系中，一个变量的值可以由另一个变量的值通过一个明确的函数 $$y=f(x)$$ 唯一确定。我们输入一个特定的 $$x$$ 值，就会得到一个确定的 $$y$$ 值。
例如，圆的面积公式：
$$A=\pi r^2$$
其中：
- $$A$$：代表圆的【面积】。
- $$\pi$$：代表圆周率，一个常数 (约等于 3.14159)。
- $$r$$：代表圆的【半径】。 在这个例子中，圆的面积 $$A$$ 完全取决于半径 $$r$$。一旦半径 $$r$$ 确定，面积 $$A$$ 就唯一确定了，因为半径是影响圆面积的唯一因素。
### 2. 不确定性关系 (又称为 相关关系)
这种关系中，一个变量的值不能被另一个变量的值唯一确定，但两者之间存在某种统计上的关联。一个特定的 $$x$$ 值可能对应多个可能的 $$y$$ 值，因为 $$x$$ 并非影响 $$y$$ 的唯一因素。
例如：
- **身高和体重**：同样身高的人，体重不一定相同。虽然我们不能根据一个具体的身高值确定一个具体的体重值，但总体趋势是，身高增加，体重的平均水平也会随之上升。
- **广告投入和销售额**
- **学习时间和考试成绩**
这些例子都表明 $$x$$ 和 $$y$$ 之间存在关系，但这种关系不是确定性的。
**本章我们主要关注的是【不确定性的关系】，即【相关关系】。**
## 二、相关关系的分类 (通过散点图观察)
散点图 (我们在第三章讲过) 是描述两个数值型变量之间关系的图形工具，通过散点图，我们可以大体观察到变量之间的关系形态和关系强弱。
从形态上来看，相关关系可以分为：
- **线性相关 (Linear Correlation)**：
    - 散点图中的各个观测点近似地分布在一条直线的周围。
    - 这是我们本章重点关注的关系类型。
    - **正相关 (Positive Correlation)**:
        - $$x$$ 和 $$y$$ 的变化方向大体相同。当 $$x$$ 增加时，$$y$$ 大体上也随之增加；当 $$x$$ 减少时，$$y$$ 大体上也随之减少。散点图中的点从左下向右上倾斜。
    - **负相关 (Negative Correlation)**:
        - $$x$$ 和 $$y$$ 的变化方向大体相反。当 $$x$$ 增加时，$$y$$ 大体上随之减少；当 $$x$$ 减少时，$$y$$ 大体上随之增加。散点图中的点从左上向右下倾斜。
- **完全相关 (Perfect Correlation)**：
    - 散点图中的各个观测点都精确地落在同一条直线上。
    - 这实际上是一种确定性关系或函数关系。
- **非线性相关 (Non-linear Correlation) / 曲线相关 (Curvilinear Correlation)**：
    - 散点图中的各个观测点近似地分布在一条曲线的周围。
- **没有相关关系 (No Correlation) / 不相关**：
    - 散点图中的观测点分散杂乱，没有表现出任何明显的趋势或规律。
**我们本章关注的是【线性相关关系】。**
## 三、相关系数 (Correlation Coefficient)
散点图能帮助我们观察变量间相关的形态和方向，但不能精确地量化变量之间线性相关的【强度】和【方向】。为此，我们引入 **相关系数** 的概念。
- **总体相关系数 (Population Correlation Coefficient)**：记为 $$\rho$$ (rho)。这是描述总体中两个变量线性相关程度的参数。
- **样本相关系数 (Sample Correlation Coefficient)**：记为 $$r$$。这是根据样本数据计算出来的，用以估计总体相关系数 $$\rho$$。
我们主要关注 **样本相关系数 $$r$$ 的计算**。
在介绍样本相关系数之前，我们先了解一下 样本协方差 (Sample Covariance)。
已知$$s_x^2$$ 和 $$s_y^2$$分别是变量 $$x$$ 和 $$y$$ 的样本方差。
### 样本协方差 ($$s_{xy}$$)
样本协方差 $$s_{xy}$$ 用来衡量两个变量的总体误差。
协方差的分子构造：
$$分子=\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$
这个分子的含义是：
- $$i$$：代表第 $$i$$ 个观测样本，从 1 到 $$n$$ (总样本量)。
- $$x_i$$：第 $$i$$ 个样本的 $$x$$ 变量的观测值。
- $$\bar{x}$$：变量 $$x$$ 的所有观测值的样本均值 (即 $$\frac{\sum x_i}{n}$$)。
- $$(x_i - \bar{x})$$：$$x_i$$ 对其均值 $$\bar{x}$$ 的离均差 (deviation from the mean)。
- $$y_i$$：第 $$i$$ 个样本的 $$y$$ 变量的观测值。
- $$\bar{y}$$：变量 $$y$$ 的所有观测值的样本均值 (即 $$\frac{\sum y_i}{n}$$)。
- $$(y_i - \bar{y})$$：$$y_i$$ 对其均值 $$\bar{y}$$ 的离均差。
- $$(x_i - \bar{x})(y_i - \bar{y})$$：对第 $$i$$ 个样本，将其 $$x$$ 的离均差与 $$y$$ 的离均差相乘。
- $$\sum_{i=1}^{n}$$：将所有 $$n$$ 个样本点计算得到的离均差乘积进行求和。
**协方差分子项的几何解释 (象限分析)：**
想象一个散点图，以 $$(\bar{x},\bar{y})$$ 为原点，可以将平面划分为四个象限：
- **第一象限**：点 $$(x_i,y_i)$$ 位于均值点的右上方。
    - $$x_i > \bar{x} \implies (x_i - \bar{x}) > 0$$
    - $$y_i > \bar{y} \implies (y_i - \bar{y}) > 0$$
    - 因此，$$(x_i - \bar{x})(y_i - \bar{y}) > 0$$ (正值)。这可以看作是以 $$(x_i - \bar{x})$$ 和 $$(y_i - \bar{y})$$ 为边的矩形面积。
- **第三象限**：点 $$(x_i,y_i)$$ 位于均值点的左下方。
    - $$x_i < \bar{x} \implies (x_i - \bar{x}) < 0$$
    - $$y_i < \bar{y} \implies (y_i - \bar{y}) < 0$$
    - 因此，$$(x_i - \bar{x})(y_i - \bar{y}) > 0$$ (正值，因为负负得正)。这也对应一个矩形的面积。
- **第二象限**：点 $$(x_i,y_i)$$ 位于均值点的左上方。
    - $$x_i < \bar{x} \implies (x_i - \bar{x}) < 0$$
    - $$y_i > \bar{y} \implies (y_i - \bar{y}) > 0$$
    - 因此，$$(x_i - \bar{x})(y_i - \bar{y}) < 0$$ (负值)。这对应一个负的矩形面积。
- **第四象限**：点 $$(x_i,y_i)$$ 位于均值点的右下方。
    - $$x_i > \bar{x} \implies (x_i - \bar{x}) > 0$$
    - $$y_i < \bar{y} \implies (y_i - \bar{y}) < 0$$
    - 因此，$$(x_i - \bar{x})(y_i - \bar{y}) < 0$$ (负值)。这对应一个负的矩形面积。
求和 $$\sum(x_i - \bar{x})(y_i - \bar{y})$$ 的过程，实际上是一个正负抵消的过程。
- 如果大部分数据点落在第一、三象限，则和为正。
- 如果大部分数据点落在第二、四象限，则和为负。
样本协方差公式：
$$s_{xy} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$
其中：
- $$s_{xy}$$：样本协方差。
- $$\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$：如上所述，是离均差乘积之和。
- $$n-1$$：自由度。在样本统计中，使用 $$n-1$$ 而不是 $$n$$ 作为分母，是为了得到总体协方差的无偏估计。
**协方差的意义：**
- 如果 $$s_{xy} > 0$$：说明正方有优势，即数据点主要分布在第一、三象限。这表明 $$x$$ 和 $$y$$ 之间存在【正相关】关系 ( $$x$$ 增加时 $$y$$ 倾向于增加， $$x$$ 减少时 $$y$$ 倾向于减少)。
- 如果 $$s_{xy} < 0$$：说明负方有优势，即数据点主要分布在第二、四象限。这表明 $$x$$ 和 $$y$$ 之间存在【负相关】关系 ( $$x$$ 增加时 $$y$$ 倾向于减少， $$x$$ 减少时 $$y$$ 倾向于增加)。
- 如果 $$s_{xy} \approx 0$$：说明正负基本抵消， $$x$$ 和 $$y$$ 之间可能没有线性相关关系。
协方差的局限性：
协方差本身可以表示 $$x,y$$ 线性相关的【方向】 (正或负)，但它【不能很好地量化线性相关的程度】。
原因是：协方差的取值会受到变量 $$x$$ 和 $$y$$ 的计量单位 (量纲) 的影响。
例如，如果 $$x$$ 是身高 (米)，$$y$$ 是体重 (千克)，计算得到一个协方差值。如果将身高单位改为厘米，体重单位改为克，那么协方差的数值会发生巨大变化，尽管身高和体重之间的实际相关程度并没有改变。
因此，协方差不方便直接用于比较不同数据集或不同变量之间的相关强度。
### 样本皮尔逊相关系数 ($$r$$)
为了克服协方差受量纲影响的缺点，并得到一个标准化的、可以比较的相关性度量，我们引入了 **皮尔逊相关系数 (Pearson Correlation Coefficient)**，通常用 $$r$$ 表示。
其核心思想是：对 $$x$$ 和 $$y$$ 变量分别进行 **标准化 (Standardization)**，然后计算标准化后变量的协方差。
标准化的操作：
一个变量的标准化值 (z-score) 是指该变量的原始值减去其均值，再除以其标准差。
- 对 $$x_i$$ 进行标准化： $$z_{xi} = \frac{x_i - \bar{x}}{s_x}$$ 其中：
    - $$z_{xi}$$：第 $$i$$ 个 $$x$$ 观测值的标准化值。
    - $$s_x$$：变量 $$x$$ 的样本标准差， $$s_x = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}}$$.
- 对 $$y_i$$ 进行标准化： $$z_{yi} = \frac{y_i - \bar{y}}{s_y}$$ 其中：
    - $$z_{yi}$$：第 $$i$$ 个 $$y$$ 观测值的标准化值。
    - $$s_y$$：变量 $$y$$ 的样本标准差， $$s_y = \sqrt{\frac{\sum(y_i - \bar{y})^2}{n-1}}$$.
标准化后的数据具有均值为0，标准差为1的特性。
样本相关系数 $$r$$ 的公式 (基于标准化数据)：
用标准化后的数据 $$z_{xi}$$ 和 $$z_{yi}$$ 计算协方差，即可得到样本相关系数 $$r$$。
(注意：标准化后变量的均值为0，所以 $$(z_{xi} - \bar{z}_x)$$ 变为 $$(z_{xi} - 0) = z_{xi}$$)
$$r = \frac{\sum_{i=1}^{n} z_{xi} z_{yi}}{n-1}$$
将 $$z_{xi}$$ 和 $$z_{yi}$$ 的定义代入上式：
$$r = \frac{\sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{s_x}\right) \left(\frac{y_i - \bar{y}}{s_y}\right)}{n-1}$$
由于 $$s_x$$ 和 $$s_y$$ 是常数 (与求和的索引 $$i$$ 无关)，可以将它们从求和符号中提到分母上：
$$r = \frac{1}{(n-1)s_x s_y} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$$
观察上式，$$\frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$ 正是样本协方差 $$s_{xy}$$。 所以，样本相关系数 $$r$$ 可以表示为：
$$r = \frac{s_{xy}}{s_x s_y}$$
这个公式非常重要，它表明：样本相关系数 $$r$$ 是 $$x$$ 和 $$y$$ 的样本协方差除以它们各自的样本标准差的乘积。
- $$r$$：样本皮尔逊相关系数。
- $$s_{xy}$$： 变量 $$x$$ 和 $$y$$ 的样本协方差。
- $$s_x$$： 变量 $$x$$ 的样本标准差。
- $$s_y$$： 变量 $$y$$ 的样本标准差。
由于标准化消除了量纲的影响，$$r$$ 的取值范围在 **-1到+1之间** (包含)。
- $$r = +1$$：表示完全正线性相关。
- $$r = -1$$：表示完全负线性相关。
- $$r = 0$$：表示没有线性相关关系 (注意，可能存在非线性关系)。
- $$r$$ 的绝对值越接近1，表示线性相关程度越强；越接近0，表示线性相关程度越弱。
样本相关系数 $$r$$ 的另一种常用计算公式 (基于原始数据展开)：
将 $$s_{xy}$$、$$s_x$$ 和 $$s_y$$ 的定义代入 $$r = \frac{s_{xy}}{s_x s_y}$$：
$$s_{xy} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$
$$s_x = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}}$$
$$s_y = \sqrt{\frac{\sum(y_i - \bar{y})^2}{n-1}}$$
代入后：
$$r = \frac{\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1}}{\sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}} \sqrt{\frac{\sum(y_i - \bar{y})^2}{n-1}}}$$
分子分母中的 $$n-1$$ (对于分母来说是 $$\sqrt{n-1} \times \sqrt{n-1} = n-1$$) 可以约掉，得到：
$$r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$
其中：
- **分子** $$\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$：$$x$$ 的离均差与 $$y$$ 的离均差的乘积之和，也称为 $$x$$ 和 $$y$$ 的【离差积和】(Sum of products of deviations)。
- **分母第一部分** $$\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$：$$x$$ 的【离差平方和的平方根】(Square root of sum of squared deviations for x)。
- **分母第二部分** $$\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}$$：$$y$$ 的【离差平方和的平方根】(Square root of sum of squared deviations for y)。
教材上常用的计算公式 (基于实际计算)：
上述公式虽然定义得很清楚，但在实际计算中，每次都计算离均差较繁琐。教材上通常会提供一个变形后的公式，更直接使用原始数据进行计算：
$$r = \frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{[n\sum x_i^2 - (\sum x_i)^2][n\sum y_i^2 - (\sum y_i)^2]}}$$
我们来理解这个公式的各个组成部分及其推导：
- $$n$$：样本点的数量 (sample size)。
- $$\sum x_i y_i$$：将每一对 ($$x_i, y_i$$) 的乘积相加。
- $$\sum x_i$$：全部 $$x$$ 值的总和。
- $$\sum y_i$$：全部 $$y$$ 值的总和。
- $$\sum x_i^2$$：将每一个 $$x$$ 值平方后相加。
- $$(\sum x_i)^2$$：先将所有 $$x$$ 值相加，然后对总和进行平方。
- $$\sum y_i^2$$：将每一个 $$y$$ 值平方后相加。
- $$(\sum y_i)^2$$：先将所有 $$y$$ 值相加，然后对总和进行平方。
**公式推导：**
1. 分子推导：$$\sum (x_i - \bar{x})(y_i - \bar{y})$$
原式 $$= \sum (x_i y_i - x_i \bar{y} - \bar{x} y_i + \bar{x}\bar{y})$$
$$= \sum x_i y_i - \bar{y}\sum x_i - \bar{x}\sum y_i + n\bar{x}\bar{y}$$
因为 $$\bar{x} = \frac{\sum x_i}{n}$$ 和 $$\bar{y} = \frac{\sum y_i}{n}$$，所以 $$\sum x_i = n\bar{x}$$ 和 $$\sum y_i = n\bar{y}$$。 代入得：
$$= \sum x_i y_i - n\bar{x}\bar{y} - n\bar{x}\bar{y} + n\bar{x}\bar{y}$$
$$= \sum x_i y_i - n\bar{x}\bar{y}$$
将 $$\bar{x} = \frac{\sum x_i}{n}$$ 和 $$\bar{y} = \frac{\sum y_i}{n}$$ 代入：
$$= \sum x_i y_i - n \left(\frac{\sum x_i}{n}\right) \left(\frac{\sum y_i}{n}\right)$$
$$= \sum x_i y_i - \frac{(\sum x_i)(\sum y_i)}{n}$$
通分得到：
$$= \frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{n}$$
2. 分母中的一项推导 (例如 $$\sum (x_i - \bar{x})^2$$)：
原式 $$= \sum (x_i^2 - 2x_i\bar{x} + \bar{x}^2)$$
$$= \sum x_i^2 - 2\bar{x}\sum x_i + n\bar{x}^2$$
代入 $$\sum x_i = n\bar{x}$$：
$$= \sum x_i^2 - 2\bar{x}(n\bar{x}) + n\bar{x}^2$$
$$= \sum x_i^2 - 2n\bar{x}^2 + n\bar{x}^2$$
$$= \sum x_i^2 - n\bar{x}^2$$
将 $$\bar{x} = \frac{\sum x_i}{n}$$ 代入：
$$= \sum x_i^2 - n\left(\frac{\sum x_i}{n}\right)^2$$
$$= \sum x_i^2 - n\frac{(\sum x_i)^2}{n^2}$$
$$= \sum x_i^2 - \frac{(\sum x_i)^2}{n}$$
通分得到：
$$= \frac{n\sum x_i^2 - (\sum x_i)^2}{n}$$
同理，对于 $$y$$ 变量：
$$\sum (y_i - \bar{y})^2 = \frac{n\sum y_i^2 - (\sum y_i)^2}{n}$$
3. 代回到 $$r$$ 的定义式 $$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2} \sqrt{\sum(y_i - \bar{y})^2}}$$:
$$r = \frac{\frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{n}}{\sqrt{\frac{n\sum x_i^2 - (\sum x_i)^2}{n}} \sqrt{\frac{n\sum y_i^2 - (\sum y_i)^2}{n}}}$$
$$r = \frac{\frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{n}}{\frac{\sqrt{[n\sum x_i^2 - (\sum x_i)^2][n\sum y_i^2 - (\sum y_i)^2]}}{\sqrt{n \cdot n}}}$$
$$r = \frac{\frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{n}}{\frac{\sqrt{[n\sum x_i^2 - (\sum x_i)^2][n\sum y_i^2 - (\sum y_i)^2]}}{n}}$$
分子分母的 $$n$$ 约掉，得到最终的计算公式：
$$r = \frac{n\sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{[n\sum x_i^2 - (\sum x_i)^2][n\sum y_i^2 - (\sum y_i)^2]}}$$
这就是教材上这个常用计算公式的来历。这个公式避免了计算均值和离均差的中间步骤，可以直接用原始数据汇总其统计量进行计算，在实际操作中更加便捷。