# 第11章：数值型变量之间关系的测量

大家好，今天呢，我们就进入第11章，这是最后四章当中最重要，也是最后四章当中最难的一章。我们在第九章和第十章分别介绍了分类变量之间关系的测量、分类自变量与数值型因变量之间关系的测量。那这一章呢？我们就介绍数值型变量之间相关关系的测量。

那两个数值型变量之间的关系呢？我们分为两种：

1.  **确定性的关系** (又称为 **函数关系**)
    比如 $y=f(x)$，那么这个 $f$ 就是对 $x$ 的某种处理。我们丢一个特定的 $x$ 进去，那么就会出来一个对应的 $y$。
    比如，圆的面积公式 $A = \pi r^2$。那圆的面积呢，取决于半径；半径确定，面积就确定了。确定一个 $r$ u就会有一个对应的面积，因为这个半径是影响圆的面积的唯一的因素。

2.  **不确定性关系** (又称为 **相关关系**)
    比如身高和体重，同样的身高，体重不一定相同。也就是说，你给我一个具体的身高值，我是不能确定具体的体重值的。但是呢，如果身高变高，平均来说体重是上升的；或者说大体上来说，如果身高变高，体重是上升的。那也就是说这个 $x$ 和这个 $y$ 之间，它是有关系的，但是这个关系并不是确定性的关系。一个特定的 $x$ 并没有一个特定的 $y$ 与之对应，因为 $x$ 并不是 $y$ 的唯一的影响因素。那么影响体重的因素，除了身高以外，还有很多。后面的这两对 $xy$ 也是一样的，存在不确定的关系。

这是我们关注的关系的类型：**不确定性的关系**，又称为**相关关系**。

但是呢，相关关系呢，又有不同的分类，我们也不是全部都关注。我们通过散点图来看一下。散点图呢，我们在第三章讲过，是描述两个数值型变量之间关系的，通过散点图，我们可以大体观察到变量之间的关系形态，还有关系的强弱。

那从形态上来看：
* **第一列的两张图**：$x$ 和 $y$ 的关系近似的表现为一条直线，那这种关系呢？我们称为 **线性相关 (Linear Correlation)**。
* **第二列的两张图**：各个观测点呢，都落在了同一条直线上。这种关系呢，我们称为 **完全相关 (Perfect Correlation)**。完全相关其实就是确定性关系了，就是函数关系了。
* **中间下方的一张图**：两个变量的关系呢，近似的表现为一条曲线，我们就称为 **非线性相关 (Non-linear Correlation)**，或者曲线相关。
* **最后一张图**：那这个观测点是分散的，没有规律的表示，变量间 **没有相关关系 (No Correlation)**，不相关。

在线性相关当中（也就是第一列的两张图当中）：
* 第一张图 $x$ 和 $y$ 的变化方向是相同的，大体上呢，呈现出同增同减这样一个关系，我们称这种线性相关为 **正相关 (Positive Correlation)**。
* 第二张图当中呢，$x$ 和 $y$ 的变化方向是相反的，它是异增一减。也就是说 $x$ 上升的时候 $y$ 大体是下降的，$x$ 下降的时候呢 $y$ 大体是上升的，那这种关系呢？称为 **负相关 (Negative Correlation)**。

那我们关注的是 **线性相关关系**，也就是在这张图的第一列的两张图。

在这两个散点图当中呢，我们是能够大概观察到变量间的相关的形态和方向的。比如，第一张图，它的这个相关的形态是线性相关，方向呢是正相关。那么第二张图呢？它也是线性相关，方向呢是负相关。但是呢，它不能量化变量之间的相关强度，所以呢，我们介绍一个概念，叫做 **相关系数 (Correlation Coefficient)**，用来量化数值型变量之间线性相关的方向和程度。

那么相关系数分为总体相关系数和样本相关系数。
* 总体相关系数，我们记为 $\rho$ (rho)。
* 样本相关系数呢，我们记为 $r$。

我们主要关注样本相关系数的计算。

呃，这两个（$s_x^2$ 和 $s_y^2$）分别是 $x$ 和 $y$ 的样本方差，这个是我们熟悉的内容。
下面这个 $s_{xy}$ 它是 $x$ 和 $y$ 的 **样本协方差 (Sample Covariance)**。

那这个协方差它的构造呢？我们看分子：
$$ \text{分子} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$
它是对每一个样本点都计算一个 $x$ 的离均差 ($x_i - \bar{x}$)，计算一个 $y$ 的离均差 ($y_i - \bar{y}$)，然后将这两个离均差相乘，最后求和。

我们看左边这张图（假设有一张散点图）：
它是样本数据的散点图。垂直的虚线是 $\bar{x}$（也就是 $x$ 的均值所在）。那么，水平的虚线呢是 $\bar{y}$（也就是 $y$ 的均值所在）。这两条虚线就把平面划分为四个象限：一、二、三、四。

* **第一个象限** 上的点，比如说这里的坐标是 $(x_i, y_i)$。那么 $x_i$ 和 $y_i$ 它分别都大于这个 $\bar{x}$ 和 $\bar{y}$。那这段的距离实际上就是 $x_i - \bar{x}$。那么，这个距离呢就是 $y_i - \bar{y}$。那么，离均差的乘积 $(x_i - \bar{x})(y_i - \bar{y})$ 实际上就是这个矩形的面积。也就是说，在第一个象限内的点，由于它的 $x_i > \bar{x}$，$y_i > \bar{y}$，所以这两个离均差相乘，一定是大于零的。
* **第三个象限** 呢？比如说这个点 $(x_i, y_i)$。$x_i$ 是小于 $\bar{x}$，$y_i$ 也是小于 $\bar{y}$ 的。也就是如果我们计算这两个离均差，那这两个离均差都是小于零，那么它的乘积 $(x_i - \bar{x})(y_i - \bar{y})$ 恰好就大于零。这个点的离均差相乘，实际上正好就是这个矩形的面积。那么在第三个象限，离均差相乘也是大于零的。
* **第二个象限** 呢？比如这个点 $(x_i, y_i)$。在这个点上 $x_i < \bar{x}$，但是 $y_i > \bar{y}$。所以，第二个象限内离均差的乘积是小于零，是负的这个矩形的面积。
* **第四个象限** 当中，比方说这个点。$x_i > \bar{x}$，但是 $y_i < \bar{y}$，所以这两个离均差相乘是负的，是负的这个矩形的面积。

那这不同象限内的点的两个变量的离均差的乘积有正有负。然后我们对每个点的离均差乘积都求和，其实就是个正负抵消的过程。
样本协方差的公式是：
$$ s_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1} $$
它的自由度呢，是 $n-1$。除以自由度是为了排除样本量的影响。

* 那如果这个协方差为正，$s_{xy} > 0$，那就说明正方有优势（乘积为正的点主要分布在一、三象限）。一、三象限有优势，大概就是这个样子。$xy$ 呢，就是正相关。
* 如果这个协方差为负，$s_{xy} < 0$，那说明负方有优势（乘积为负的点主要分布在二、四象限）。二、四象限有优势，大概就是这个样子。$xy$ 呢就呈现出负相关的关系。

所以这个协方差本身，其实是可以表示 $x,y$ 线性相关的方向的。它可以衡量你到底是正相关还是负相关。那它能不能量化线性相关的程度呢？答案是不能。因为 $x$ 和 $y$（比如说这个 $x$ 是温度，$y$ 呢是这个病毒的存活时间），我们想观察一下这两个变量之间的相关程度的话，那如果对温度和存活时间的计量尺度发生了变化，或者使用的单位发生了变化，它都会影响这个协方差的结果。它不会影响这个协方差的符号，但是会影响协方差的取值。那么，我们想同样一组样本，如果只是量纲或者计量尺度不同，那是不会影响 $x,y$ 的相关程度的，但是如果量纲和计量尺度不同，协方差的值又会发生变化。那么，在这种情况下，表明协方差是不方便度量相关程度的。

那我们为了排除量纲和计量尺度的影响，我们首先呢，对 $x$ 和 $y$ 都进行一个分别进行标准化，然后用标准化以后的数值再计算协方差就会得到相关系数。
标准化的操作：
对 $x_i$ 进行标准化，那就是 $z_{xi} = \frac{x_i - \bar{x}}{s_x}$，其中 $s_x$ 是 $x$ 的样本标准差。
那么对 $y$ 呢也是同样的操作，$z_{yi} = \frac{y_i - \bar{y}}{s_y}$。

我们就得到了这两个标准化以后的数据。标准化以后的数据呢，按照协方差的计算公式代进去，就是样本相关系数 $r$：
$$ r = \frac{\sum_{i=1}^{n} z_{xi} z_{yi}}{n-1} $$
上面是 $\sum z_{xi} z_{yi}$，我们还要减掉标准化以后的均值，但是标准化以后的均值是零，那就是减零就不减了。
那这个时候 $z_{xi} = \frac{x_i - \bar{x}}{s_x}$ 和 $z_{yi} = \frac{y_i - \bar{y}}{s_y}$ 分别代入，我们就得到了这样一个式子：
$$ r = \frac{\sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s_x} \right) \left( \frac{y_i - \bar{y}}{s_y} \right)}{n-1} $$
也就是说我们对 $x$ 和 $y$ 先分别进行标准化，用标准化以后的数据计算协方差就会得到相关系数。
那这个相关系数呢，如果我们把这个 $s_x$ 和 $s_y$ 因为它跟 $i$ 是没有关系的，我们可以提到外面来，提到分母上：
$$ r = \frac{1}{(n-1)s_x s_y} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) $$
剩下的这个部分 $\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}$ 其实就是 $x$ 和 $y$ 的协方差 $s_{xy}$。
所以我们可以整理成这个样子：
$$ r = \frac{s_{xy}}{s_x s_y} $$
上面是协方差，下面是两个标准差的乘积，就会得到相关系数。

那如果我们把各自的标准差 $s_x = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n-1}}$ 和 $s_y = \sqrt{\frac{\sum (y_i - \bar{y})^2}{n-1}}$，以及协方差 $s_{xy} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}$ 带进来，就会得到：
$$ r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}} $$
这个上下啊，都是求和，然后呢，注意这个下面呢两个带根号。呃，然后呢，这个 $n-1$ 都消掉了，是因为上面有一个 $n-1$（在 $s_{xy}$ 的分母中），下面有两个 $\sqrt{n-1}$（分别在 $s_x, s_y$ 的分母中），相乘还是 $n-1$，也就消掉了。大家可以自己带进来去算一下。

那这个嗯，教材上呢，用的是这种形式（计算公式），我们整理一下，也能整理成这样：
$$ r = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{[n \sum x_i^2 - (\sum x_i)^2][n \sum y_i^2 - (\sum y_i)^2]}} $$
**分子推导：**
$\sum (x_i - \bar{x})(y_i - \bar{y})$
$= \sum (x_i - \bar{x})y_i - \sum (x_i - \bar{x})\bar{y}$
由于 $\bar{y}$ 是常数，且 $\sum (x_i - \bar{x}) = 0$，所以第二项为零。
$= \sum (x_i y_i - \bar{x} y_i) = \sum x_i y_i - \bar{x} \sum y_i$
将 $\bar{x} = \frac{\sum x_i}{n}$ 代入：
$= \sum x_i y_i - \frac{(\sum x_i)(\sum y_i)}{n} = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{n}$

**分母中的一项推导（例如 $\sum (x_i - \bar{x})^2$）：**
$\sum (x_i - \bar{x})^2 = \sum (x_i - \bar{x})x_i - \sum (x_i - \bar{x})\bar{x}$
第二项为零。
$= \sum (x_i^2 - \bar{x} x_i) = \sum x_i^2 - \bar{x} \sum x_i$
将 $\bar{x} = \frac{\sum x_i}{n}$ 代入：
$= \sum x_i^2 - \frac{(\sum x_i)^2}{n} = \frac{n \sum x_i^2 - (\sum x_i)^2}{n}$
同样的道理，$\sum (y_i - \bar{y})^2 = \frac{n \sum y_i^2 - (\sum y_i)^2}{n}$。

代回到 $r$ 的定义式（皮尔逊相关系数的原始形式）：
$$ r = \frac{\frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{n}}{\sqrt{\frac{n \sum x_i^2 - (\sum x_i)^2}{n}} \sqrt{\frac{n \sum y_i^2 - (\sum y_i)^2}{n}}} $$
$$ r = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{[n \sum x_i^2 - (\sum x_i)^2][n \sum y_i^2 - (\sum y_i)^2]}} $$
这就是教材上这个式子的来历。

那现在呢，我们知道了相关系数的来源和计算方法，还有教材上这个公式是怎么整理来的。

---

## 相关系数的性质

> [!INFO] 相关系数的性质
> 1.  **取值范围**：$r$ 的取值范围是 $[-1, 1]$。
>     * 如果 $r > 0$，就是正相关。
>     * 如果 $r < 0$，就是负相关。
>     * $|r|$ 越大，表明变量之间的线性相关性越强。
>     * 当 $r = -1$，两个变量呈现完全的负相关关系。
>     * 当 $r = +1$，两个变量呈现完全的正相关关系。
>     * 如果 $r = 0$，两个变量之间不存在线性关系（但可能存在非线性关系）。
>     * 在 $(-1, 0)$ 之间，呈现出负相关关系；在 $(0, 1)$ 之间，就存在正相关关系。
> 2.  **对称性**：$x$ 和 $y$ 之间的相关系数 $r_{xy}$ 等于 $y$ 和 $x$ 之间的相关系数 $r_{yx}$。
>     如果我们回过头去看公式，把 $x$ 和 $y$ 互换位置，结果是不会变的。这是它数学上对称性的体现。实际上我们在用相关系数研究变量线性相关程度的时候，并没有主次之分，$x$ 和 $y$ 的地位是相同的，我们研究的是它们互相之间的关系。
> 3.  **尺度不变性**：$r$ 的大小与 $x$ 和 $y$ 的原点以及测量尺度无关。
>     这个是因为我们介绍相关系数来源的时候是对它进行了标准化，然后又去做了协方差，所以我们就排除掉了像原点、尺度、量纲的影响。
> 4.  **线性关系度量**：$r$ 仅仅是 $x, y$ 之间 **线性关系** 的一个度量，不能用于描述非线性关系。
>     比如下面这张图（假设一张U型或倒U型散点图），$x,y$ 存在的是非常明显的非线性关系（曲线关系）。这个时候如果我们计算线性相关系数 $r$（这个 $r$ 虽然叫做相关系数，但是它是线性相关系数），那么这个值可能是 $0$ 或接近 $0$。但这并不能说明 $x,y$ 之间不存在关系，它只是不存在线性关系。
> 5.  **相关不蕴含因果**：$r$ 虽然是变量之间线性关系的一个度量，但不意味着两个变量一定有因果关系。
>     事实是这样的，我们的这个出发点、我们的目的就没奔着因果关系去。这是第一。第二个呢，比如，你家门口有两棵树，它们一起长高，如果我们每周都记录一次两棵树的高度，那么这两棵树高度的变化，相关性会非常的强，但是呢，这并不能说明因果关系。这就是我们经常说的“相关未必因果 (Correlation does not imply causation)”。

好，我们这个是相关系数的性质。

---

## 相关系数的检验 (显著性检验)

那这个相关关系的显著性检验，它是检验谁呢？它是检验 **总体相关系数 $\rho$**。那么用到的原料呢，是 **样本相关系数 $r$**。
这就好比我们计算样本均值 $\bar{x}$ 可以作为总体均值 $\mu$ 的估计，我们计算样本相关系数 $r$ 也可以作为总体相关系数 $\rho$ 的估计。那我们用基于样本均值去检验总体均值，那也可以用样本相关系数去检验总体相关系数。

因为样本抽样具有随机性，所以这个样本相关系数 $r$ 也具有随机性，有自己的分布，那么就像样本均值有自己的抽样分布是一样的道理。

那我们检验总体均值的时候，用 $\bar{x}$ 去检验 $\mu$ 的时候，是在关于总体均值的原假设成立的条件下（也就是有一个 $H_0: \mu = \mu_0$ 这样一个假定下），然后去确定 $\bar{x}$ 的分布，然后再对 $\bar{x}$ 进行标准化构造检验统计量（或者是 $z$ 统计量或者是 $t$ 统计量）。

那么类似的，我们要检验总体的相关系数 $\rho$，我们用的是样本的相关系数 $r$。那我们也要在总体相关系数的原假设之下去确定 $r$ 的抽样分布，然后对 $r$ 进行某种变换来构造检验统计量。

现在假如，我们计算的样本相关系数 $r = 0.52$。那么这个时候我们可不可以把原假设设定为 $H_0: \rho = 0.5$ 呢？那如果原假设成立，那么 $r$ 它的取值呢会集中在 $\rho = 0.5$ 附近。但是呢，由于 $r$ 的取值范围是从 $-1$ 到 $1$，那么 $\rho = 0.5$ 时，它的左边的活动范围就更大（从 $-1$ 到 $0.5$），右边的活动范围就很小（从 $0.5$ 到 $1$），以至于 $r$ 的抽样分布会呈现出一个 **左偏**。那基于这样一个左偏分布去构造检验统计量是非常困难的。
* 如果原假设 $H_0: \rho = \rho_0$ 且 $\rho_0 > 0$，那么 $r$ 的抽样分布都会呈现出一个左偏的形态。
* 如果原假设 $H_0: \rho = \rho_0$ 且 $\rho_0 < 0$（比如说 $\rho_0 = -0.5$），那么情况正好相反，$r$ 的抽样分布就会呈现出一个 **右偏** 的形态。

那不管是左偏还是右偏，对我们构造检验统计量来说都很困难。只有当我们假设 $H_0: \rho = 0$ (即总体不相关) ，在这样一个原假设之下，$r$ 的抽样分布才是对称的（或近似对称）。并且随着样本量的增加，这个分布 $r$ 的这个抽样分布会趋于正态分布。那在这种情况下才方便我们进行下一步，也就是构造检验统计量。

所以呢，我们对这个相关关系的检验，对总体相关系数的检验，就只有这一种形式：**检验总体的相关系数是否等于零**。
对应的假设：
* 原假设 $H_0: \rho = 0$ (总体中 $x$ 和 $y$ 线性无关)
* 备择假设 $H_1: \rho \neq 0$ (总体中 $x$ 和 $y$ 线性相关)

对应的检验统计量是这样一个 $t$ 统计量形式：
$$ t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} $$
它服从自由度为 $df = n-2$ 的 $t$ 分布。
临界值，那就是 $t_{\alpha/2}$ (因为 $\rho \neq 0$ 是个双侧检验)。当计算出的 $|t|$ 大于这个 $t_{\alpha/2}$ 的时候，我们就拒绝原假设 $H_0$。

那至于说这个 $t$ 统计量为什么如此构造？在这里我们就不讲了，这个跟后面的回归是有关系的。但是我们知道，这个检验统计量就是通过样本计算来的，然后呢，根据显著性水平 $\alpha$ 去确定这样一个临界值，两相比较就行了。

**例题**：
算是我们知道相关系数 $r=0.8436$，样本量 $n=25$，显著性水平 $\alpha=0.05$。
对应的临界值为 $t_{0.025, df=23} = 2.069$ (查表或计算器得到，这里原文是 $2.0687$)。
就可以计算这个检验统计量：
$$ t = \frac{0.8436 \sqrt{25-2}}{\sqrt{1 - 0.8436^2}} = \frac{0.8436 \sqrt{23}}{\sqrt{1 - 0.71166096}} = \frac{0.8436 \times 4.7958}{\sqrt{0.28833904}} \approx \frac{4.0457}{0.53697} \approx 7.534 $$
用这个值与临界值相比较， $|7.534| > 2.069$，就拒绝原假设。认为 $x$ 和 $y$ 之间存在显著的线性相关关系。

那以上呢，就是相关系数的内容。

---
---

## 一元线性回归 (Simple Linear Regression)

那我们用相关系数研究两个变量之间的线性相关程度。相关系数 $r$ 衡量的这个强度，它是 $x$ 和 $y$ 互相之间的，$x, y$ 的地位呢是相同的。而且呢，我们也没有研究过这条直线（散点图中趋势线）。

左边这张图呢，是代表总体的，我们计算了 $r$，然后对总体的相关系数 $\rho$ 做了一个检验。但是我们自始至终并没有研究过，总体当中这条直线。现在呢，我们开始关心一个变量对另外一个变量的 **影响**，比如 $x$ 对 $y$ 的影响。那么此时 $x$ 就称为 **自变量 (Independent Variable)**，而 $y$ 呢被称为 **因变量 (Dependent Variable)**。自变量和因变量在地位上就不一样了。自变量是对因变量可以起到解释说明的作用，而因变量呢是被解释被说明的那一个变量。

为了达到我们的目的，研究一个变量对另外一个变量的影响，我们现在开始关注总体当中的这条直线。呃，但是不要忘了哦，我们又只能通过样本来估计这条直线。那么这条直线到底它的是如何的一条直线？其实我们是不知道的，我们也永远不会知道。我们会通过右边这个样本数据、样本观测点去估计这条直线。

那么，为了确定一条直线，我们需要什么呢？我们需要这条直线的 **截距 (Intercept)** 和 **斜率 (Slope)**。那截距和斜率确定了，直线就确定了。所以我们通过样本来估计这条直线，比如说用样本估计的出来这样一条直线，实际上就是估计直线的截距和斜率这两个参数。

我们把总体当中这条直线写成：
$$ E(y|x) = \beta_0 + \beta_1 x $$
$\beta_0$ 呢是截距，$\beta_1$ 呢是斜率。
那现在我们描述的是这条直线，但是总体上的各个点并不在这条直线上，而是围绕着直线上下波动。也就是说，如果我们给定一个确定的 $x$（比如 $x_0$），对应的直线上的点是 $\beta_0 + \beta_1 x_0$。但是，当 $x$ 取 $x_0$ 的时候，$y$ 的值是不是都在这个点上啊？不是，它是围绕着这个点做上下的波动。也就是说，有一个 $x_0$ 是确定的值，但是并没有确定的 $y$ 在总体当中，而是这个 $y$ 围绕着这个点在做上下的波动。也就是说它不是一个完全的线性关系，它就是一个线性相关的关系。

那么，为了表达这种关系，我们在这个直线上加一个随机波动进去。也就是在 $\beta_0 + \beta_1 x$ 上加一个 $\epsilon$ (epsilon) 进去。那么这个 $\epsilon$ 是一个随机变量，来表示 $y$ 围绕着这条直线在波动。那么这一项呢，我们称为 **随机误差项 (Random Error Term)**。
然后我们让 $y$ 等于这样一个式子：
$$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$
那么这个式子就描述了总体当中所有点的情况。这个式子呢，被称为 **一元线性回归模型 (Simple Linear Regression Model)**。
所谓的一元呢，意思就是自变量只有一个。呃，如果自变量有多个，那就是多元。

那么这个 $\epsilon$ 它是不是随便一个随机变量就可以呢？呃，并不是，这个随机变量要符合一定的前提假定。

---

### 一元线性回归模型的假定

> [!IMPORTANT] 一元线性回归模型的假定
> 大家注意一下，这个一元线性回归模型是针对总体而言的。我们需要对一元线性回归模型做假定，其实就是对总体的这个线性关系做假定。做假定的目的呢，一个是为了确定适用条件，另外一个是为了估计和检验。
>
> 1.  **线性关系**：$x$ 和 $y$ 之间呢，要存在线性关系。即因变量 $y$ 的期望值是自变量 $x$ 的线性函数：$E(y_i|x_i) = \beta_0 + \beta_1 x_i$。如果 $x,y$ 之间不是线性关系，做线性回归是不合适的。
> 2.  **固定 $x$ 值**：$x$ 是固定非随机的，或者说 $x$ 的值是在重复抽样中保持固定的，或者说我们的分析是以给定的 $x$ 值为条件的。意思就是说 $x$ 的情况对我们来说是确定的，或者我们也可以理解成我们在每一个确定的 $x$ 上去观察 $y$ 的波动。这个时候 $y$ 是随机的，$x$ 是确定的。从这个模型 $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ 当中，我们也可以看出如果 $x_i$ 是确定的，而且 $\beta_0$ 和 $\beta_1$ 它作为这个总体的参数，它也是确定的（但是未知的）。我们后面又加了一个随机项 $\epsilon_i$，所以 $y_i$ 是随机的。
>
> 后面的三、四、五条都是针对这个随机误差项 $\epsilon$ 的：
> 3.  **零均值**：对于任何一个特定的 $x$ 值，$\epsilon$ 的期望（均值）等于零。即 $E(\epsilon_i) = 0$。
>     我们根据这样一个特点，对这个模型 $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ 两边同时取期望：
>     $E(y_i) = E(\beta_0 + \beta_1 x_i + \epsilon_i)$
>     $E(y_i) = E(\beta_0) + E(\beta_1 x_i) + E(\epsilon_i)$
>     由于 $\beta_0, \beta_1$ 是常数，$x_i$ 是固定的，所以 $E(\beta_0) = \beta_0$，$E(\beta_1 x_i) = \beta_1 x_i$。
>     又因为 $E(\epsilon_i) = 0$，所以：
>     $$ E(y_i) = \beta_0 + \beta_1 x_i $$
>     那这个式子在说什么呢？就是虽然对于一个给定的 $x$，$y$ 并不是确定的，但是对于一个给定的 $x$，$y$ 的 **期望** 却是在这样一条直线上。这个式子其实它描述的就是总体当中的这条直线，称为 **总体回归方程 (Population Regression Equation)**。
> 4.  **同方差 (Homoscedasticity)**：不管 $x$ 取值如何，$\epsilon_i$ 的方差是一样的，都等于 $\sigma^2$。即 $Var(\epsilon_i) = \sigma^2$。
>     那么，不管 $x$ 取值如何，$y_i$ 的方差也是一样的。因为我们计算 $y_i$ 的方差：
>     $Var(y_i) = Var(\beta_0 + \beta_1 x_i + \epsilon_i)$
>     由于 $\beta_0 + \beta_1 x_i$ 部分是确定非随机的，所以它的方差是零。
>     $Var(y_i) = Var(\epsilon_i) = \sigma^2$。
>     因为我们也可以想一下，就对于一个特定的 $x$，$y$ 的随机性的来源就只有 $\epsilon$。所以这个 $y$ 的波动性与 $\epsilon$ 的波动性一定是一样的。
> 5.  **独立性** (通常还会假定误差项之间相互独立)：任意两个观测值的误差项 $\epsilon_i$ 和 $\epsilon_j$ ($i \neq j$) 是不相关的，即 $Cov(\epsilon_i, \epsilon_j) = 0$。
> 6.  **正态性** (可选，但在进行推断时常用)：对于任何一个 $x$ 值，$\epsilon_i$ 总是服从同样一个正态分布。
>     那么之前我们还知道它的期望是零，方差是 $\sigma^2$，现在又知道它是正态分布，所以它服从的是均值为零，方差是 $\sigma^2$ 的正态分布：$\epsilon_i \sim N(0, \sigma^2)$。
>     那这样也会导致 $y_i$ 服从正态分布。因为 $y_i = (\beta_0 + \beta_1 x_i) + \epsilon_i$，前面 $(\beta_0 + \beta_1 x_i)$ 部分是不具有随机性的（对于给定的 $x_i$），然后再加一个正态分布，那就是正态分布的线性组合，它一定也是正态分布。
>     我们又知道了 $y_i$ 的均值是 $E(y_i) = \beta_0 + \beta_1 x_i$，$y_i$ 的方差是 $\sigma^2$，所以 $y_i$ 服从的正态分布的参数为：
>     $$ y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2) $$
>     也就是说呢，对于任何一个确定的 $x_i$，那么 $y_i$ 它的分布是正态分布，其中均值是 $\beta_0 + \beta_1 x_i$，然后方差是 $\sigma^2$。

那这是关于模型的假定。

我们的目的是估计中间这条直线，那么直线如何表示呢？我们刚才说过了，实际上直线的表示方法就是总体回归方程：
$$ E(y) = \beta_0 + \beta_1 x $$
* $\beta_0$ 是直线在 $y$ 轴上的截距。
* $\beta_1$ 是直线的斜率，表示 $x$ 每变动一个单位，$y$ 的 **平均** 变动值。
    它为什么是平均变动值呢？比如说现在，我们对于一个 $x_0$，那么 $E(y|x_0) = \beta_0 + \beta_1 x_0$。
    现在呢，我们让 $x_1 = x_0 + 1$，那这个时候 $E(y|x_1) = \beta_0 + \beta_1 x_1 = \beta_0 + \beta_1 (x_0+1) = \beta_0 + \beta_1 x_0 + \beta_1$。
    那现在我们看一下 $E(y|x_1) - E(y|x_0) = (\beta_0 + \beta_1 x_0 + \beta_1) - (\beta_0 + \beta_1 x_0) = \beta_1$。
    所以这个 $x$ 变动一个单位，$y$ 的 **平均** 变动是 $\beta_1$。
* $\beta_0$ 呢是截距，就是 $x=0$ 的时候，在这个位置上，这条直线与 $y$ 轴的相交的这个点，这个点就叫做 $\beta_0$。

那这是我们关于总体回归模型和回归方程的介绍。那我们的目标呢，是要估计这个方程，也就是说要估计这条直线，那就是要估计这两个参数 $\beta_0$ 和 $\beta_1$。我们使用的原料呢是样本观察点。那我们要利用样本观察值得到这样一条直线，就是要得到这两个参数的估计值 $\hat{\beta_0}$ (beta0 hat) 和 $\hat{\beta_1}$ (beta1 hat)。那么这个等式：
$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x $$
就称为 **估计的回归方程 (Estimated Regression Equation)** 或样本回归方程。那么有了这个方程以后呢，对于一个确定的 $x$，那就会有一个 $y$ 的估计值 $\hat{y}$ 产生。

那我们下面就是要想办法来估计这两个参数，也就是估计这条直线的截距和斜率。

---

### 最小二乘法 (Method of Least Squares)

我们先不管这条直线我们是如何去得到的，也先不管得到的这个直线它的截距是几、斜率是几。那这个样本观测点 $(x_i, y_i)$，它总是围绕在这个直线的周围嘛。那对于任何一个观测点 $(x_i, y_i)$，其中 $y_i$ 是这个点纵坐标的一个真实值，它对应的估计值在哪里呢？在估计的回归直线上，为 $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$。

真实值与估计值之间的差 $e_i = y_i - \hat{y_i}$ 被称为 **残差 (Residual)**。
残差是有正有负的。我们就用残差的平方 $e_i^2 = (y_i - \hat{y_i})^2$ 来表示每个点与其估计值之间的这个波动或者说是距离。
如果我们的样本量是 $n$，那就有 $n$ 个残差，也会有 $n$ 个残差的平方。那么这 $n$ 个残差的平方和，我们称为 **残差平方和 (Sum of Squared Residuals, SSE)**：
$$ SSE = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - (\hat{\beta_0} + \hat{\beta_1} x_i))^2 $$
我们画这样一条直线，就会产生一个残差平方和。那如果我再做另一条直线，它就会产生另外一个残差平方和。那么，在样本确定的情况下，直线变化（即 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 变化），残差平方和就会变化。

在我们可能得到的直线中，有千千万万无穷无尽的直线。从残差平方和的角度，我们肯定是希望得到那条能够使残差平方和 **最小** 的直线。因为残差平方和最小，意味着说我现在得到的这条直线能够使得点围绕着直线达到一个最紧密的程度。那么我们要使得残差平方和最小，就等价于要找到使得残差平方和最小的这两个估计值 $\hat{\beta_0}$ 和 $\hat{\beta_1}$。

$SSE$ 是 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 的函数。这个平方和它没有最大值，但是它有最小值。最小值在这个导数等于零处取得。所以呢，我们现在分别对 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 这两个参数求偏导，并且让这两个偏导都等于零：
1.  $\frac{\partial SSE}{\partial \hat{\beta_0}} = -2 \sum (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) = 0$
2.  $\frac{\partial SSE}{\partial \hat{\beta_1}} = -2 \sum x_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) = 0$

就会得到两个关于 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 的方程（称为正规方程 Normal Equations）：
1.  $\sum y_i = n \hat{\beta_0} + \hat{\beta_1} \sum x_i$
2.  $\sum x_i y_i = \hat{\beta_0} \sum x_i + \hat{\beta_1} \sum x_i^2$

两个方程，两个未知数。那我们就可以通过解方程求解这两个未知数。
解得：
$$ \hat{\beta_1} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{n \sum x_i^2 - (\sum x_i)^2} $$
$$ \hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x} $$
这个关系 $\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$ 表明我们估计的那条直线一定会通过一个点，就是 $(\bar{x}, \bar{y})$。这也是这个直线的一个很重要的特征。

**$\hat{\beta_1}$ 的另一种表达形式（与相关系数和标准差有关）：**
$$ \hat{\beta_1} = r \frac{s_y}{s_x} $$
其中 $s_x$ 和 $s_y$ 是样本标准差。

**系数的含义：**
假设我们现在估计出来一个方程就是 $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$。
* $\hat{\beta_0}$ (截距项)：表示当 $x=0$ 时，$\hat{y}$ 是多大。但是这个 $\hat{\beta_0}$ 我们对它并没有怎么关注，因为第一点这个线性关系的体现其实是主要体现在斜率上；第二点呢，这个截距有很多时候它是不能对现实的意义做一个合理的解释的，有的时候甚至这个解释会很荒谬。比如如果说我们根据样本计算出来的估计出来的这条直线（身高为 $x$，体重为 $y$），$\hat{\beta_0}$ 可能表示当身高 $x=0$ 的时候，体重 $\hat{y}$ 是个负数，这种解释其实很荒谬。所以这个 $\hat{\beta_0}$，你知道它是 $x$ 取零的时候在 $y$ 轴上的截距就可以了。
* $\hat{\beta_1}$ (斜率项)：关键是对这个 $\hat{\beta_1}$ 的解释。这个解释是这样的：**在其他条件不变的情况下，$x$ 每增加一个单位，$y$ 平均增加 (或减少) $\hat{\beta_1}$ 个单位。**
    具体到我们这个例子中（原文是 $0.037895$），如果 $\hat{\beta_1} = 0.037895 > 0$，$x$ 每增加一个单位，$y$ 就平均增加 $0.037895$ 个单位。那如果 $x$ 每减少一个单位呢？$y$ 平均就减少 $0.037895$ 个单位。那 $x$ 和 $y$ 它的变动方向是一致的。
    什么叫“每增加一个单位”呢？就是说我不管你 $x$ 从哪个值上增加，你比方说你 $x$ 本来等于 $2$，你从 $2$ 上增加一个单位，$y$ 的平均增加是 $\hat{\beta_1}$ 个单位。那么，你如果 $x$ 从 $4$ 上增加一个单位，那 $y$ 平均增加呢还是 $\hat{\beta_1}$ 个单位。这叫每增加一个单位，$y$ 的平均的增加呢是一样的，因为它总是沿着这个斜率走。

那我们使用残差平方和最小求截距和斜率估计值的方法呢，就叫做 **最小二乘法 (Least Squares Method)**。

---

### 拟合优度 (Goodness of Fit)

接接下来呢，我们来看一下对我们求解的这个方程的一种评价，就是拟合优度。

#### 1. 判定系数 ($R^2$ 或 $r^2$) (Coefficient of Determination)

左边这张图呢，有两条线：黑色的这条线就是我们的估计的方程 $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$。那么蓝色的水平的虚线呢是 $y$ 的均值 $\bar{y}$。
我们用 $y$ 的离均差平方和，称为 **总平方和 (Total Sum of Squares, SST)**，可以衡量 $y$ 的总波动：
$$ SST = \sum (y_i - \bar{y})^2 $$
但是呢，这每个离均差 $(y_i - \bar{y})$ 又可以分为两个部分。
$(y_i - \bar{y}) = (y_i - \hat{y_i}) + (\hat{y_i} - \bar{y})$
* $(y_i - \hat{y_i})$ 是残差 $e_i$。
* $(\hat{y_i} - \bar{y})$ 是回归可解释的偏差。

可以证明（在最小二乘法假定下，且模型包含截距项时）：
$$ \sum (y_i - \bar{y})^2 = \sum (\hat{y_i} - \bar{y})^2 + \sum (y_i - \hat{y_i})^2 $$
即：
$$ SST = SSR + SSE $$
其中：
* $SST = \sum (y_i - \bar{y})^2$：总平方和，度量了因变量 $y$ 的总变异。
* $SSR = \sum (\hat{y_i} - \bar{y})^2$：**回归平方和 (Sum of Squares due to Regression)**，度量了可由回归模型解释的 $y$ 的变异部分。
* $SSE = \sum (y_i - \hat{y_i})^2$：**残差平方和 (Sum of Squared Errors/Residuals)**，度量了不能由回归模型解释的 $y$ 的变异部分（剩余平方和）。

我们找的这个点 $(x_i, y_i)$，它脱离了均值 $\bar{y}$ 这么远（$y_i - \bar{y}$）。它之所以能够脱离均值这么远是为什么呢？
* 有一部分就是 $x$ 可以解释的，是哪一部分呢？是 $(\hat{y_i} - \bar{y})$ 这一部分。这个部分是 $x$ 它在这个位置，所以引起的它的估计值 $\hat{y_i}$ 能够到这个位置。这个部分是 $x$ 可以解释的。
* 那么 $(y_i - \hat{y_i})$ 这个部分呢？它是真实的值减去估计值，这个部分就是 $x$ 没有办法解释的。$x$ 只能解释你为什么从均值到了估计值这个位置，而它不能解释你为什么从估计值又跑出去这么远。这个就是 $x$ 不能解释的了。

我们用 $x$ 可以解释的这部分平方和 ($SSR$) 比上总的平方和 ($SST$)，就是 $x$ 可以解释的那一部分的比例，称为 **判定系数 $R^2$**：
$$ R^2 = \frac{SSR}{SST} = \frac{\sum (\hat{y_i} - \bar{y})^2}{\sum (y_i - \bar{y})^2} $$
根据 $SST = SSR + SSE$，判定系数 $R^2$ 又可以写成这种形式：
$$ R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{\sum (y_i - \hat{y_i})^2}{\sum (y_i - \bar{y})^2} $$
$R^2$ 的取值呢是 $[0, 1]$。它的值越大，说明 $x$ 能够解释的 $y$ 的总波动占比就越大，拟合优度呢就越好。
在一元线性回归中，$R^2 = r^2$，即判定系数等于相关系数 $r$ 的平方。

**关于 $\sum (y_i - \hat{y_i})(\hat{y_i} - \bar{y}) = 0$ (交乘项为零的说明):**
根据我们求偏导的式子（正规方程），可以推导出 $\sum e_i = \sum (y_i - \hat{y_i}) = 0$ 和 $\sum x_i e_i = \sum x_i (y_i - \hat{y_i}) = 0$。
那么交乘项 $\sum (y_i - \hat{y_i})(\hat{y_i} - \bar{y}) = \sum e_i (\hat{\beta_0} + \hat{\beta_1} x_i - \bar{y})$
$= \hat{\beta_0} \sum e_i + \hat{\beta_1} \sum x_i e_i - \bar{y} \sum e_i$
由于 $\sum e_i = 0$ 和 $\sum x_i e_i = 0$，所以这个交乘项等于零。这就是为什么总平方和可以分解成这两个平方和的原因。

#### 2. 估计标准误差 ($s_e$ 或 $\hat{\sigma}$) (Standard Error of the Estimate)

从总体的回归模型 $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ 当中，我们可以得到这个随机误差 $\epsilon_i = y_i - (\beta_0 + \beta_1 x_i)$。随机误差的方差是 $\sigma^2$。
但是我们在样本当中是得不到随机误差 $\epsilon_i$ 的，我们能够得到的是残差 $e_i = y_i - \hat{y_i}$。
那现在我们要估计随机误差的方差 $\sigma^2$，就可以使用残差的样本方差来做这样一个估计，称为 **均方残差 (Mean Squared Error, MSE)**：
$$ MSE = \frac{SSE}{n-k-1} = \frac{\sum (y_i - \hat{y_i})^2}{n-2} $$
(在一元线性回归中，参数个数 $k+1=2$，所以自由度是 $n-2$。或者说自变量个数 $k=1$，自由度是 $n-k-1 = n-1-1=n-2$。)
这个 $MSE$ 作为 $\sigma^2$ 的一个无偏估计。
然后我们对它开根号，就得到 **估计标准误差 $s_e$** (或记为 $\hat{\sigma}$)，作为 $\sigma$ 的一个估计：
$$ s_e = \sqrt{MSE} = \sqrt{\frac{\sum (y_i - \hat{y_i})^2}{n-2}} $$
当然了，我们做完这个方程的估计以后，我们肯定是希望这个残差的平方和 $SSE$ 它越小越好，$MSE$ 或者 $s_e$ 它越小，说明这个我们的样本观测点与直线围绕的越紧密，就说明拟合优度越好。
但是这个单纯的计算 $MSE$ 或者 $s_e$ 本身的意义其实并不大（不像 $R^2$ 那样有直接的百分比解释），但是呢，这个 $MSE$ (或 $s_e$) 对我们后续做检验和区间估计是很有帮助的。

那这个是两种关于估计的方程的评价：一种呢是 $R^2$ 判定系数，它的取值是 $[0, 1]$，越大越好；那么第二种呢是估计标准误差 $s_e$，它当然是越小越好。

---

### 回归方程的检验

下面呢，我们就看我们对方程的两种检验。

#### 1. 线性关系检验 (F检验 / 方差分析)

这个检验呢，跟方差分析的检验呢也很相似。它的出发点呢是从全局从整体考虑问题。
原假设 $H_0: \beta_1 = 0$ (自变量 $x$ 与因变量 $y$ 之间没有线性关系)
备择假设 $H_1: \beta_1 \neq 0$ (自变量 $x$ 与因变量 $y$ 之间存在线性关系)

能够用自变量解释的部分的均方 ($MSR$) 与不能用自变量解释的部分的均方 ($MSE$) 做比值，就会得到一个 $F$ 统计量。
* 回归均方 $MSR = \frac{SSR}{k}$ (这里 $k$ 是自变量个数，一元回归中 $k=1$)
* 残差均方 $MSE = \frac{SSE}{n-k-1} = \frac{SSE}{n-2}$

检验统计量：
$$ F = \frac{MSR}{MSE} $$
在 $H_0$ 成立的条件下，$F$ 统计量服从分子自由度为 $k=1$，分母自由度为 $n-k-1 = n-2$ 的 $F$ 分布。即 $F \sim F(1, n-2)$。

这是一个 **右侧检验**。临界值呢是 $F_{\alpha, 1, n-2}$。
因为如果原假设不成立（即 $\beta_1 \neq 0$），$SSR$ 会变大，从而 $MSR$ 变大，$F$ 值也倾向于变大。
如果我们计算得到的 $F$ 统计量比这个临界值要大，我们就拒绝原假设，表明变量之间是存在显著的线性关系的。

> [!NOTE] F检验与t检验的关系
> 在一元线性回归中，对 $\beta_1=0$ 的 $F$ 检验等价于对 $\beta_1=0$ 的双侧 $t$ 检验。具体来说，$F = t^2$，其中 $t$ 是用于检验 $\beta_1=0$ 的 $t$ 统计量，其自由度为 $n-2$。

#### 2. 回归系数的检验 (t检验)

那我们要学习的呢，还是关于这个斜率系数 $\beta_1$ 它是不是等于零的这样一个检验。
原假设 $H_0: \beta_1 = 0$
备择假设 $H_1: \beta_1 \neq 0$ (也可以是单侧 $H_1: \beta_1 > 0$ 或 $H_1: \beta_1 < 0$)

这个在一元回归当中，线性关系的检验和系数的检验（对 $\beta_1$）的原假设是一样的，只不过出发点不一样。刚才是我们是从总体方差分解出发，现在呢我们就不是从整体出发了，我们就是单独看 $\beta_1$ 是不是等于零。

这种检验呢，其实跟我们之前学过的假设检验（比如对总体均值 $\mu$ 的检验）是很相似的。我们检验 $\mu$ 的时候，我们是用 $\mu$ 的估计值 $\bar{x}$，从 $\bar{x}$ 的分布开始去构造检验统计量。现在我们估计这个 $\beta_1$，检验这个 $\beta_1$ 是不是等于零，我们也要从 $\beta_1$ 的估计值 $\hat{\beta_1}$ 出发，那去构造检验统计量。

$\hat{\beta_1}$ 它其实是一个随机变量，因为它最后计算的结果是取决于样本的，样本又有随机性，所以这个估计值 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 它都是随机变量，它有自己的分布。

可以证明（在回归模型基本假定下）：
* $\hat{\beta_1}$ 的期望：$E(\hat{\beta_1}) = \beta_1$ ($\hat{\beta_1}$ 是 $\beta_1$ 的无偏估计)
* $\hat{\beta_1}$ 的方差：$Var(\hat{\beta_1}) = \sigma^2_{\hat{\beta_1}} = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}$
* $\hat{\beta_1}$ 的标准误 (Standard Error of $\hat{\beta_1}$)，用 $s_e = \sqrt{MSE}$ 替换未知的 $\sigma$：
    $$ s_{\hat{\beta_1}} = \frac{s_e}{\sqrt{\sum (x_i - \bar{x})^2}} = \sqrt{\frac{MSE}{\sum (x_i - \bar{x})^2}} $$

如果误差项 $\epsilon_i$ 服从正态分布，则 $\hat{\beta_1}$ 也服从正态分布：
$\hat{\beta_1} \sim N\left(\beta_1, \frac{\sigma^2}{\sum (x_i - \bar{x})^2}\right)$。

检验统计量 (当 $\sigma^2$ 未知时，用 $MSE$ 估计)：
$$ t = \frac{\hat{\beta_1} - \beta_{1,0}}{s_{\hat{\beta_1}}} $$
其中 $\beta_{1,0}$ 是原假设中 $\beta_1$ 的值 (这里是 $0$)。
所以，检验 $H_0: \beta_1 = 0$ 的 $t$ 统计量为：
$$ t = \frac{\hat{\beta_1}}{s_{\hat{\beta_1}}} $$
该统计量服从自由度为 $n-2$ 的 $t$ 分布。

**检验步骤：**
1.  提出假设：$H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ (或 $>0$, $<0$)。
2.  计算检验统计量 $t = \frac{\hat{\beta_1}}{s_{\hat{\beta_1}}}$。
3.  确定显著性水平 $\alpha$ 和临界值 (例如双侧检验为 $t_{\alpha/2, n-2}$)。
4.  做出决策：如果计算出的 $|t| > t_{\alpha/2, n-2}$ (对于双侧检验)，则拒绝 $H_0$，认为系数显著不为零。

**示例：** (接续之前相关系数的例子，假设已算出回归结果)
假设 $\hat{\beta_1} = 0.037895$，$s_{\hat{\beta_1}} = 0.005030$，$n=25$，$\alpha=0.05$。
临界值 $t_{0.025, 23} = 2.069$ (原文为 $2.0687$)。
检验统计量：
$$ t = \frac{0.037895}{0.005030} \approx 7.534 $$
因为 $|7.534| > 2.069$，所以拒绝原假设 $H_0$，认为系数 $\beta_1$ 显著不为零。

那这个是两种检验：一个是线性关系检验 (F检验)，一个是回归系数的检验 (t检验)。

---

### 使用回归方程进行预测

那如果我们估计得到的方程通过了线性关系检验，通过了系数的显著性检验（表明这个系数显著不为零，并且变量 $x$ 和 $y$ 之间存在这个显著的线性关系），那我们就可以用我们估计的方程做预测。
所谓的预测呢，就是给我们一个特定的 $x$ 的值 (记为 $x_0$)，然后对 $y$ 进行点估计和区间估计。

但是这个估计呢要分情况：
1.  对 $y$ 的 **均值 $E(y_0) = \beta_0 + \beta_1 x_0$** 的估计。
2.  对 $y$ 的 **个别值 $y_0 = \beta_0 + \beta_1 x_0 + \epsilon_0$** 的预测。

#### 点估计

假如左边是总体和总体回归方程，右边是样本数据。我们通过样本数据估计的方程是 $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$。
现在如果给我们 $x = x_0$。
* 对总体均值 $E(y_0)$ 做点估计：
    我们直接把 $x=x_0$ 带到这个估计的方程当中，得到一个 $y$ 的估计值 $\hat{y_0}$，作为对这个总体均值 $E(y_0)$ 的估计。
    $$ \hat{y_0} = \hat{\beta_0} + \hat{\beta_1} x_0 $$
* 对 $y$ 的个别值 $y_0$ 做点估计：
    我们还是把 $x=x_0$ 带到这个估计方程当中，又出来一个 $y$ 的估计值 $\hat{y_0}$，作为对 $y_0$ 的估计值。
    这是因为 $\epsilon_0$ 的期望是 $0$，所以对个别值的最佳点预测值与对均值的点估计值相同。

所以在对 $y$ 的点估计当中，不管是对均值的点估计还是对个别值的点估计，都是同样的处理：给定一个 $x=x_0$，直接把 $x_0$ 带到这个方程，得到 $\hat{y_0}$。

#### 区间估计

区间估计就比较复杂，我们要知道 $\hat{y_0}$ 它的分布才行。

1.  **对均值 $E(y_0)$ 的置信区间 (Confidence Interval for the Mean Value of y)**

    $\hat{y_0} = \hat{\beta_0} + \hat{\beta_1} x_0$ 也是一个随机变量。
    * $E(\hat{y_0}) = E(y_0) = \beta_0 + \beta_1 x_0$
    * $\hat{y_0}$ 的标准误 $s_{\hat{y_0}}$ (或 $s_{m}$，m for mean)：
        $$ s_{\hat{y_0}} = s_e \sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}} $$
        其中 $s_e = \sqrt{MSE}$。

    $E(y_0)$ 的 $(1-\alpha)$ 置信区间为：
    $$ \hat{y_0} \pm t_{\alpha/2, n-2} \cdot s_{\hat{y_0}} $$
    $$ \hat{y_0} \pm t_{\alpha/2, n-2} \cdot s_e \sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}} $$

2.  **对个别值 $y_0$ 的预测区间 (Prediction Interval for an Individual Value of y)**

    个别值 $y_0$ 它不固定，它是围绕着这个固定的 $E(y_0)$ 在做波动（$y_0 = E(y_0) + \epsilon_0$）。
    我们在预测这个个别值的时候，就要把这个额外的波动 $\epsilon_0$ 加进去。这个波动由 $\sigma^2$ (用 $s_e^2 = MSE$ 估计) 来体现。
    预测误差 $y_0 - \hat{y_0}$ 的方差为 $Var(y_0 - \hat{y_0}) = \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}\right)$。
    $\hat{y_0}$ 的标准误 (用于个别值预测，或 $s_{ind}$，ind for individual)：
    $$ s_{y_0 - \hat{y_0}} = s_e \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}} $$

    $y_0$ 的 $(1-\alpha)$ 预测区间为：
    $$ \hat{y_0} \pm t_{\alpha/2, n-2} \cdot s_{y_0 - \hat{y_0}} $$
    $$ \hat{y_0} \pm t_{\alpha/2, n-2} \cdot s_e \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}} $$

**比较：**
我们看置信区间和预测区间，它俩相比，同样的置信水平，同样的样本，估计出来的同样的直线：
* **个别值的预测区间要比均值的置信区间要宽**，因为它这里（标准误的根号下）多了一个 $1$。
* 对于置信区间也好，还是预测区间也好，它的宽度的变化呢还会受到 $(x_0 - \bar{x})^2$ 这个值的影响。也就是说你给定的这个 $x_0$ 它离 $\bar{x}$ 越远，那么这个平方就会越大，以至于这个区间就会越宽。那什么时候能达到最窄呢？就是你离 $\bar{x}$ 越近，它就越窄。当这个 $x_0 = \bar{x}$ 的时候，$(x_0 - \bar{x})^2$ 这一项就等于零了就消失了。这个时候，置信区间和预测区间都达到了各自的最窄。

